{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "854fe42de7eb5738d1bd7c9fdec1965e",
     "grade": false,
     "grade_id": "cell-c71e0219a0180e0e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<h2 align=\"center\">Model Validation and Selection.</h2>\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>CS-EJ3211 Machine Learning with Python 29.05.-17.07.2023</center>\n",
    "<center>Aalto University (Espoo, Finland)</center>\n",
    "<center>fitech.io (Finland)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4fd26f58901f60a5a32a8624ae5c9136",
     "grade": false,
     "grade_id": "cell-97da75c1d52f0687",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When applying machine learning to prediction problems, our main objective is to find a model that can accurately predict the label of data points for which it is yet unknown. To this end, we want to estimate a model's predictive capability on data points that have not been used for training the model. This is also frequently referred to as the **generalization capability** of the model.\n",
    "\n",
    "In the previous rounds, we established that ML models learn by searching for the predictor in the hypothesis space that minimizes the average loss on a set of **training data**. Also, we considered how to calculate the **training error** of a ML model, i.e., the error of the model on the training data. \n",
    "\n",
    "Unfortunately, the training error is a poor proxy for what we wish to measure - the predictive performance on a new and unlabeled data. Based on this observation, we are faced with two questions:\n",
    "\n",
    "1. How can we better estimate the performance of a trained model on a  new and unlabeled data? Such estimates are needed to assess whether a ML model is suitable for practical use and to select the best model for deployment out of many alternatives.\n",
    "\n",
    "\n",
    "2. Is it possible to alter a ML model to learn a predictor function with good generalization capabilities? This is in contrast to learning the function that minimizes the average training loss/error.\n",
    "\n",
    "In this notebook, you will find answers to both of these questions. We will first consider different **model validation** techniques for reliably estimating model performance on new data. Later, we will look at **regularization** techniques that enable ML models to learn predictor functions with better generalization capabilities by minimizing a penalized average loss during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "08753d52c8136d6cffd92ac396c388ee",
     "grade": false,
     "grade_id": "cell-49483a7aad5aa158",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Learning goals\n",
    "\n",
    "\n",
    "In this notebook, you will\n",
    "\n",
    "* learn that the training error is a poor quality measure for a ML model \n",
    "* learn that the validation error is a more useful quality measure for a ML model \n",
    "* learn how to choose between different models using the validation error\n",
    "* learn about regularization as a soft variant of model selection. \n",
    "\n",
    "## Reading Material \n",
    "\n",
    "* Chapter Chapter 6 & 7 of course book [Machine Learning: The basics.](https://github.com/alexjungaalto/MachineLearningTheBasics/blob/master/MLBasicsBook.pdf)  \n",
    "\n",
    "## Additional Material \n",
    "\n",
    "* [Video lecture](https://www.youtube.com/watch?v=MyBSkmUeIEs) of Prof. Andrew Ng on model validation and selection\n",
    "* [Short video](https://www.youtube.com/watch?v=TIgfjmp-4BA) on K-Fold Cross validation from Udacity\n",
    "* [Video lecture](https://www.youtube.com/watch?v=QjOILAQ0EFg) of Prof. Andrew Ng on regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d2eaa858317a2b5d1494e931cd35fe0a",
     "grade": false,
     "grade_id": "cell-90c66bf37e8ad022",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "## What is model validation?\n",
    "\n",
    "Model validation is the process of evaluating the predictive capability of a ML model on new data points. By using good model validation techniques, we can estimate the performance of a ML model with a reasonable confidence prior to its use in practice. Before we dive into the details of model validation, we will briefly consider **overfitting** in order to show why model validation is necessary.\n",
    "\n",
    "Suppose that we want to predict a label $y$ of data points based on their features $\\mathbf{x}$. In order to do this, we choose a ML model and train it on a set of labelled data by minimizing the average loss of its predictions. A naive approach we might then take to estimate the performance of the trained model is to calculate its training error, which we assume to be equivalent to the average loss. Unfortunately, this approach is highly problematic and leads to overly optimistic estimates of the model's predictive capabilities.\n",
    "\n",
    "The training error is a poor estimate of a model's predictive performance because it is calculated on the same dataset that was used to train the model. Since this dataset only represents a small sample of all available data points, it is likely that a predictor that minimizes the average loss on the training set happens to fit this dataset particularly well. Consequently, the average loss on the training set will, in general, overestimate the performance of the model. This phenomenon is referred to as **overfitting**, because the trained model fits the training set overly well.\n",
    "\n",
    "While some degree of overfitting is typically present when applying a ML model to a prediction problem, it is a particularly significant issue when applying ML models with large hypothesis spaces that contain complex predictor functions (in relation to the size of the dataset). In such cases, a model that seemingly fits the data very well might turn out to perform very poorly on data points not in the training set. This type of situation is exemplified in the figure below.\n",
    "\n",
    "<img src=\"../../../coursedata/3_ModelValSel/overfitting.png\" width=500/>\n",
    "\n",
    "The figure visualizes a dataset containing $m=20$ data points, along with two different predictor functions that were obtained by training two ML models on the four data points indicated by the orange crosses. The green predictor belongs to a standard linear regression model, while the red predictor belongs to a more complex, 4th degree polynomial model. The key observation regarding the predictors is that while the more complex model fits the data used for training very well, it has a much poorer fit on the rest of the data than the simpler linear regression model.\n",
    "\n",
    "While the figure above seems like an extreme example due to miniscule number of data points in the training set, it nonetheless makes an important point. If we do not have the capacity to validate a model's performance on data points outside the training set, we will likely  overestimate the predictive capability of complex models due to overfitting. This sets us up for a great let-down when applying the models in practice. Furthermore, we will end up selecting overly complex models over simpler ones when choosing between many candidate models. \n",
    "\n",
    "As a final note, it is worth emphasizing that the optimal complexity of the predictor function is generally dependent on the dataset's size. A deep neural network might generalize well when trained on a huge dataset, whereas even a linear model using many features might be prone to severe overfitting on a small dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ab2e95da75092235b164a38d25b36c23",
     "grade": false,
     "grade_id": "cell-d82c01565964839f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Basic Model Validation\n",
    "\n",
    "The simplest scheme for validating the predictive performance of a ML model is to train the model on only some of the available labelled data, and use the rest to validate the performance of the trained model. Typically, the dataset used for training is referred to as the **training set**, and the data used for validation is referred to as the **validation set**. The image below visualizes a dataset split into a training and validation set.\n",
    "\n",
    "<img src=\"../../../coursedata/3_ModelValSel/SplitValTrain.jpg\" width=500/>\n",
    "\n",
    "Let us repeat the scheme above in slightly more detail. Assume that we have a dataset $\\mathbb{X}$ at our disposal, and that we wish to train a ML model that can predict the label $y$ of data points based on their features $\\mathbf{x}$. To be able to later validate the performance of the model, we will first split the dataset $\\mathbb{X}$ into a training set denoted by $\\mathbb{X}^{(t)}$ and a validation set denoted by $\\mathbb{X}^{(v)}$. The recommended size of the validation set is somewhat context dependent, but a common choice is 20-30% of the entire dataset.\n",
    "\n",
    "After splitting the dataset, we train our model on the training set by minimizing the average loss. Finally, we calculate the average prediction error on the validation set in order to estimate the performance of our model on new data points. The average prediction error on the validation set is referred to as the **validation error**. \n",
    "\n",
    "Let's apply this validation scheme to the fictional prediction problem from the previous round, in which we sought to find the best model from a selection of models using a different number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c8ff9c81a78de1062a1cc2c3598bc2f7",
     "grade": false,
     "grade_id": "cell-de358afd2c4fac99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## The Problem \n",
    "\n",
    "We revisit the fictional scenario considered in notebook 2, in which you are an intern at a real estate fund in Helsinki and have been assigned a task of developing a ML model for predicting an apartment price of different neighborhoods in Helsinki. The following excerpt serves to refresh your memory on the problem formulation:\n",
    "\n",
    "\"*The staff in your group have built a dataset containing features of 20 different neighborhoods. These features include the average number of rooms in the apartments, the percentage of buildings constructed before 1970 in the neighborhood, as well as eight experimental features designed by your colleagues in research. In addition, the fund's domain experts have valued a representative sample of apartments in each of the 20 neighborhoods and have arrived at a reliable estimate for the median apartment price in each one. This valuation has cost the fund a fortune, but the investment will pay off if your ML model can use this information to accurately predict the median apartment value of other neighborhoods at a negligible cost.*\n",
    "\n",
    "*Let us model the problem of predicting the median home values as a machine learning problem. The data points in this problem are the different neighborhoods, and the label of these is the median apartment price. The features of the neighborhoods were already explicitly referred to in the previous paragraph. Still, it might be worth repeating that the features are properties belonging specifically to the data points.*\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1f24426d0d823740dd2a25f4347eb803",
     "grade": false,
     "grade_id": "cell-3d2949507bb9fa14",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='handsondata'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "### Demo. Load the data.\n",
    "    \n",
    "In the code snippet below, we create a function for loading the feature matrix `X` and label vector `y` for the apartment value problem. The function returns the feature matrix `X` of shape `(m,n)` and the label vector `y` of shape `(m,)`.\n",
    "    \n",
    "In most cases, it is not necessary to create a separate function for loading the data if the procedure is this simple. However, it is convenient in this case as we will reload the data before many tasks to make sure that the dataset has not been modified. In case you would have modified the dataset by mistake, you could receive wrong answers on student tasks despite your solutions being correct.\n",
    "    \n",
    "At the end of the cell, we use the newly defined function to load the data and print the shape of the feature matrix and label vector.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a0787d8d745ab33217dbbfd8d388fe29",
     "grade": false,
     "grade_id": "cell-711d85b7cf810763",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10) (20,)\n"
     ]
    }
   ],
   "source": [
    "# Import basic libraries needed \n",
    "import numpy as np \n",
    "import pandas as pd  \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def load_housing_data(n=10):\n",
    "    # Load dataframe from csv\n",
    "    df = pd.read_csv(\"../../../coursedata/3_ModelValSel/helsinki_apartment_prices.csv\", index_col=0)  \n",
    "    \n",
    "    # Extract feature matrix and label vector from dataframe\n",
    "    X = df.iloc[:,:n].to_numpy()\n",
    "    y = df.iloc[:,-1].to_numpy().reshape(-1)\n",
    "    \n",
    "    return X, y\n",
    "    \n",
    "# Load the housing data\n",
    "X, y = load_housing_data()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2d9def8ef245639a595b3f434540fc1a",
     "grade": false,
     "grade_id": "cell-ddfcf8b06137c570",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Linear Predictors \n",
    "\n",
    "Recall that we previously calculated the training errors for a selection of linear regression models that use a different number of features for prediction. We will consider the same models in this notebook, but we will use more sophisticated methods to validate the performance of each model, and to select the one that outputs the best predictions on new data points. Altogether, we have 10 candidate models, each using the first $r$ features $x_1, x_2, \\ldots, x_r$ in the original dataset, with $r$ ranging from 1 to 10, to predict the label $y$ of data points. \n",
    "\n",
    "The hypothesis space for a linear regression model using the first $r$ features consists of predictor functions of the form\n",
    "\n",
    "\\begin{equation*}\n",
    "h^{(\\mathbf{w})}(\\mathbf{x}) = w_0 + \\mathbf{w}^{T} \\mathbf{x} = w_0 + \\sum_{i=1}^r w_i x_i,\n",
    "\\end{equation*}\n",
    "\n",
    "where the feature weights $\\mathbf{w}=\\big(w_1, w_2, \\ldots, w_r \\big)^T$ and the intercept term $w_0$ are the model parameters. Thus, the search for the best model amounts to the search for the best $r$.\n",
    "\n",
    "We will start out by repeating the experiment of calculating the training errors for the different candidate models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eaf1e24610ae3e57d9470d8e7fb2803b",
     "grade": false,
     "grade_id": "cell-0aaf4cb2c4109eb2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<a id='trainModel'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "### Demo. Varying Number of Features\n",
    "    \n",
    "The following code snippet computes the training error $E_{\\rm train}(r)$ for each choice of $r$. For each particular value $r=1,\\ldots,n$, the best linear predictor $h(\\mathbf{x})$ is found using the  function `.fit()` of the `LinearRegression` class in scikit-learn.\n",
    "\n",
    "[Documentation of the LinearRegression class in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) \n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "38f21146afad18912265270827a8f68c",
     "grade": false,
     "grade_id": "cell-acd5c9243afcd36f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n = 10                         # maximum number of features used \n",
    "\n",
    "X, y = load_housing_data(n=n)  # read in 20 data points using n features \n",
    "linreg_error = np.zeros(n)     # vector for storing the training errors\n",
    "\n",
    "for i in range(n): \n",
    "    reg = LinearRegression(fit_intercept=True)       # create an object for linear predictors\n",
    "    reg.fit(X[:,:(i+1)], y)      # find best linear predictor (minimize training error)\n",
    "    pred = reg.predict(X[:,:(i+1)])    # compute predictions of best predictors \n",
    "    linreg_error[i] = mean_squared_error(y, pred)    # compute training error \n",
    "\n",
    "plot_x = np.linspace(1, n, n, endpoint=True)    # plot_x contains grid points for x-axis (1,...,n)\n",
    "\n",
    "# Plot training error E(r) as a function of feature number r\n",
    "plt.rc('legend', fontsize=14)    # Set font size for legends\n",
    "plt.rc('axes', labelsize=14)     # Set font size for axis labels\n",
    "plt.figure(figsize=(8,4))        # Set figure size\n",
    "plt.plot(plot_x, linreg_error, label='$E(r)$', color='red')\n",
    "plt.xlabel('# of features $r$')\n",
    "plt.ylabel('training error $E(r)$')\n",
    "plt.title('training error vs number of features', fontsize=16)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 10) (20,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape,y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.047443799833164\n",
      "7.693037221421177\n",
      "7.65579243575858\n",
      "7.291167133467584\n",
      "7.195610008943261\n",
      "5.627667557538581\n",
      "5.539152202714609\n",
      "5.428788218860019\n",
      "5.383015692980808\n",
      "5.382999431264581\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n = 10                         # maximum number of features used \n",
    "\n",
    "X, y = load_housing_data(n=n)  # read in 20 data points using n features \n",
    "linreg_error = np.zeros(n)     # vector for storing the training errors\n",
    "\n",
    "for i in range(n): \n",
    "    reg = LinearRegression(fit_intercept=True)       # create an object for linear predictors\n",
    "    reg.fit(X[:,:(i+1)], y)      # find best linear predictor (minimize training error)\n",
    "    pred = reg.predict(X[:,:(i+1)])    # compute predictions of best predictors \n",
    "    linreg_error[i] = mean_squared_error(y, pred)    # compute training error \n",
    "    print(linreg_error[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ea525ab47ec17b26036eb6c01e0704e6",
     "grade": false,
     "grade_id": "cell-4fba2c040fb160f3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Let's Interpret the Results!\n",
    "\n",
    "In the previous notebook, we were tempted to conclude that the model using all of the features is the best one based on the graph above. However, you might already have come to doubt this conclusion based on the issues considered earlier. \n",
    "\n",
    "As we already discussed, it is not a good idea to use the training error to assess the performance of the optimal predictor since this predictor was found by minimizing the average loss on the training set. \n",
    "\n",
    "Furthermore, the more features (larger $r$) we use, the better we will be able to fit the training data $\\mathbb{X}^{(t)}$. However, this does not necessarily lead to better performance on new data. A complex model with too many features (large $r$) might only fit the training data very well, and generalize poorly to new data.\n",
    "\n",
    "Consider the case of $r=m_{\\rm train}$, i.e., the number of features is the same as the number of labeled data points in the training set. Under very mild conditions it can be shown that in this case there always exists a linear predictor $h(\\mathbf{x})=\\mathbf{w}^{T} \\mathbf{x}$ such that $y^{(i)} = h(\\mathbf{x}^{(i)})$, i.e., the training error is exactly zero! \n",
    "A better way to evaluate the quality of a predictor is presented next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9aeee760e26a778a43a669765b28e90c",
     "grade": false,
     "grade_id": "cell-8e508ee65304e99f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Using the Validation Error\n",
    "\n",
    "In this setting, the training error is a bad measure for the performance of the models since it will always favor the ones using more features (larger $r$). A better way to estimate the performance is to use the validation error\n",
    "\n",
    "\\begin{equation}\n",
    "E_{\\rm val}(r) = (1/m_{v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(r)}_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}, \n",
    "\\end{equation} \n",
    "\n",
    "which is the MSE of the true and predicted labels of the data points in the validation set.\n",
    "\n",
    "Since a lower validation error suggests better predictive capabilities on new data points, the best model is defined as the one resulting in the smallest validation error. Consequently, we should choose the model with the lowest validation error when selecting between multiple different models for solving an ML problem.\n",
    "\n",
    "Next, we will explore how proper model validation changes our model choice in the example problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d5f034b3f0c189c9b50f44af1cfafbae",
     "grade": false,
     "grade_id": "cell-8a7b319a106cbd05",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='splitTestandValidationfunction'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Student Task 3.1. Generate Training and Validation Set.\n",
    "   \n",
    "Use the function `train_test_split()` in Scikit-learn's `model_selection` module to split the apartment data into a training and validation set. \n",
    "\n",
    "The function takes as input the original feature matrix `X` and label vector `y` and outputs the feature matrices and label vectors of the training- and validation sets in the order `X_train, X_val, y_train, y_val`. Moreover, the function takes additional input parameters that specify how the data should be split. In this case, you should set the parameters `test_size=0.2` and `random_state=2`. The parameter `test_size` indicates the proportion of the dataset that is used as the validation(/test) set.\n",
    "    \n",
    "For more information on the function, we refer to its [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ac84c1ad96470421c9ae47a1bb8ab76",
     "grade": false,
     "grade_id": "cell-9cc67382efb4ce19",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 10) (16,)\n",
      "\n",
      " (4, 10) (4,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split    # Import train_test_split function\n",
    "\n",
    "# Load the data\n",
    "X, y = load_housing_data(n=10) \n",
    "\n",
    "### STUDENT TASK ###\n",
    "# Compute the training and validation sets\n",
    "# X_train, X_val, y_train, y_val = ...\n",
    "# YOUR CODE HERE\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(\"\\n\",X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45a8cce90d7ef35c3df2c350bfb6c5b4",
     "grade": false,
     "grade_id": "cell-2fbabdcefb77f271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# Check dimensions of train and validation vectors\n",
    "assert (X_train.shape == (16, 10)) & (X_val.shape == (4, 10)), \"The shape of feature matrices are wrong!\"\n",
    "assert (y_train.shape == (16,)) & (y_val.shape == (4,)), \"The shape of label vectors are wrong!\"\n",
    "\n",
    "assert (int(X_train.sum()) == 1262) & (int(X_val.sum()) == 219), \"The values of feature matrices are wrong!\"\n",
    "assert (int(y_train.sum()) == 355) & (int(y_val.sum()) == 102), \"The values of label vectors are wrong!\"\n",
    "\n",
    "print('Sanity checks passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f148e55c93a46e5213a8e0ada9f48f7",
     "grade": true,
     "grade_id": "cell-0092a1fd6f111058",
     "locked": true,
     "points": 0.125,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "323953fd9b2c1e837a01883379dc7026",
     "grade": true,
     "grade_id": "cell-e25e465a8854f6b2",
     "locked": true,
     "points": 0.125,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2851fa688173205e704c73718ab76672",
     "grade": true,
     "grade_id": "cell-63073617e17ff284",
     "locked": true,
     "points": 0.125,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ad544fd9d4280519714de405e172a4f",
     "grade": true,
     "grade_id": "cell-5b91d6759bb28559",
     "locked": true,
     "points": 0.125,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "942ffc38bec17bb2735fc36a595dad98",
     "grade": false,
     "grade_id": "cell-428dadef568d5aef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='trainValErrorsfunction'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Student Task 3.2. Compute Training and Validation Error. \n",
    "    \n",
    "Complete the function `get_train_val_errors(X_train, X_val, y_train, y_val, n_features)` that returns the training and validation errors of linear predictors for each choice of $r=1,\\ldots,n$. Please use `fit_intercept=True`. \n",
    "    \n",
    "The training errors should be stored in a numpy array `err_train` of shape $(n,)$ and the validation errors should be stored in the numpy array `err_val` of shape $(n,)$. The first entries of `err_train` and `err_val` should correspond to the models with number of features $r=1$, second - with two features $r=2$, etc. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "20b127f08020384e646916905fb7b0e2",
     "grade": false,
     "grade_id": "cell-7791084dee96529b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_train_val_errors(X_train, X_val, y_train, y_val, n_features):  \n",
    "    #linreg_error = np.zeros(n)\n",
    "    err_train = np.zeros(n_features)  # Array for storing training errors\n",
    "    err_val = np.zeros(n_features)    # Array for storing validation errors\n",
    "    \n",
    "    for i in range(n_features):    # Loop over the number of features r \n",
    "        ### STUDENT TASK ###\n",
    "        # YOUR CODE HERE\n",
    "        reg = LinearRegression(fit_intercept=True)\n",
    "        #fit linear model #reg.fit(X[:,:(i+1)], y)\n",
    "        reg.fit(X_train[:,:(i+1)], y_train)\n",
    "        # Calculate the predicted labels of the data points in the training set # using training data to find predicted y\n",
    "        pred_train = reg.predict(X_train[:,:(i+1)]) \n",
    "        err_train[i] = mean_squared_error(y_train, pred_train)\n",
    "        pred_val = reg.predict(X_val[:,:(i+1)]) \n",
    "        err_val[i] = mean_squared_error(y_val, pred_val)\n",
    "        \n",
    "    return err_train, err_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "85f1f1e8aa68e1e4b223335e7832d672",
     "grade": false,
     "grade_id": "cell-204b560d3ab8a920",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity checks passed!\n"
     ]
    }
   ],
   "source": [
    "# max number of features\n",
    "n = 10\n",
    "\n",
    "# Calculate training and validation errors using get_train_val_errors() function\n",
    "err_train, err_val = get_train_val_errors(X_train, X_val, y_train, y_val, n)\n",
    "\n",
    "# Perform some sanity checks on the results\n",
    "assert err_train.shape == (n,), \"numpy array err_train has wrong shape\"\n",
    "assert err_val.shape == (n,), \"numpy array err_val has wrong shape\"\n",
    "print('Sanity checks passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b90a27eacbf04bd8a0ab4b8c43b4149",
     "grade": true,
     "grade_id": "cell-c1430b345ba97bfc",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c3188aab2ef743a9c8707f68d7284f4",
     "grade": true,
     "grade_id": "cell-4503c74413171356",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6ce57059e9f3d19e5a6d2eb37300793",
     "grade": false,
     "grade_id": "cell-802d3729eb036c9f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Plot resulting training and validation errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0494fe11a2a07be8a2d3b0cd807afebe",
     "grade": false,
     "grade_id": "cell-64473b41dbc92dfa",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation errors for the different number of features r\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1, n + 1), err_train, color='black', label=r'$E_{\\rm train}(r)$', marker='o')  # Plot train error\n",
    "plt.plot(range(1, n + 1), err_val, color='red', label=r'$E_{\\rm val}(r)$', marker='x')  # Plot validation error\n",
    "\n",
    "plt.title('Training and validation error for different number of features', fontsize=16)    # Set title\n",
    "plt.ylabel('Empirical error')    # Set label for y-axis\n",
    "plt.xlabel('r features')    # Set label for x-axis\n",
    "plt.xticks(range(1, n + 1))  # Set the tick labels on the x-axis to be 1,...,n\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e0f451f8456c36085bbe584fdfa0e8f",
     "grade": false,
     "grade_id": "cell-2d3b038c92f6014f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you have completed the task correctly, you should see plot similar to this one:\n",
    "\n",
    "<img src=\"../../../coursedata/3_ModelValSel/train_val_error.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52f3171e303368b9c3dc8fbac4158daf",
     "grade": false,
     "grade_id": "cell-61ecf20008a16c4b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the figure, we can see that the validation error behaves very differently in comparison to the training error. The validation error obtains its minimum value at $r=2$ whereas the training error is monotonously decreasing with respect to $r$. It is clear that the training error is misleading as a measure of a model's performance on new data points.\n",
    "\n",
    "While the validation error in the figure above exposes the uselessness of the training error, the simple validation scheme we used is not perfect either. If the dataset used for training and validation is not very large, the amount of data points in the validation set might be insufficient for reliably describing the distribution of data points in general. Consequently, the average error on the validation set might not accurately reflect the performance of the model on new data points.\n",
    "\n",
    "For example, the validation error for $r=2$ is much lower than the training error in the figure above. Since ML models have a tendency to overfit the training set, the error on the validation set should not be lower than the training error if both sets accurately represent the distribution of the data. Therefore, we have a strong reason to believe that the validation set used in the task above just happens to fit the trained predictor for $r=2$ very well.\n",
    "\n",
    "Fortunately, the reliability of the validation error can be easily improved by averaging multiple validation errors calculated using different training and validation sets. This approach is called $K$-fold cross-validation, and is presented next in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1b7fc44a495121578777e612243f3921",
     "grade": false,
     "grade_id": "cell-a417d9495eb3f40e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## K-fold Cross-Validation\n",
    "\n",
    "When the amount of data available for training and validation is limited, there is a trade-off between overfitting the training set and the reliability of the validation error. On one hand, a smaller training set makes the models more prone to overfitting, but on the other, a smaller validation set leads to a less reliable estimate of the model's generalization capability.\n",
    "\n",
    "This problem can largely be avoided by using a more sophisticated validation method called **$K$-fold cross-validation**. In $K$-fold cross-validation, the data is first split into $K$ approximately equally sized subsets. Then, the subsets are used to calculate $K$ different estimates of the validation error, so that the $i$:th estimate is calculated using the $i$:th subset as the validation set and the remaining $K-1$ subsets as the training set. Finally, the $K$ estimates are averaged to obtain a final estimate of the validation error.\n",
    "\n",
    "As an example, a diagram of  5-fold cross-validation is depicted below. For each split, the subset used as the validation set is indicated with blue color box and the training set with green color. For each split we fit a model to training set (green) and compute validation error on validation set (blue). Five validation errors then averaged to get 5-fold cross-validation error.\n",
    "\n",
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=600/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f07b8795135dc89694c9e4f1f3e83795",
     "grade": false,
     "grade_id": "cell-22532d7547837140",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='kfold'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "    \n",
    "### Demo. Creating K data splits in sklearn.\n",
    "    \n",
    "The code snippet below shows how to use a `KFold` object in scikit-learn to iterate through `K` train/validation splits of the dataset `X`.\n",
    "    \n",
    "On initialization the `KFold` object is given the number of data splits `K` as an argument to the parameter `n_splits`. The Python [generator function](https://docs.python.org/3.8/glossary.html#term-generator) `KFold.split(X)` can then be used to iterate through the pairs of training and validation indices. \n",
    "\n",
    "For an array `idx` of indices, the data points in X corresponding to these indices can be obtained by `X[idx,:]`. We can use this to obtain the training and validation sets given the indices of the datapoints in the respective sets.\n",
    "\n",
    "For more information, see the scikit-learn [documentation of KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4abfb2935781e46b9da65f07cefa0075",
     "grade": false,
     "grade_id": "cell-289719f33c888e47",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Indices for validation set: [0 1 2 3]\n",
      "Indices for training set: [ 4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "Iteration 2:\n",
      "Indices for validation set: [4 5 6 7]\n",
      "Indices for training set: [ 0  1  2  3  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "Iteration 3:\n",
      "Indices for validation set: [ 8  9 10 11]\n",
      "Indices for training set: [ 0  1  2  3  4  5  6  7 12 13 14 15 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "Iteration 4:\n",
      "Indices for validation set: [12 13 14 15]\n",
      "Indices for training set: [ 0  1  2  3  4  5  6  7  8  9 10 11 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "Iteration 5:\n",
      "Indices for validation set: [16 17 18 19]\n",
      "Indices for training set: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import KFold class from scikitlearn library\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 5    # Specify the number of folds to split data into\n",
    "kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "# For all splits, print the validation and training indices\n",
    "iteration = 0\n",
    "for train_indices, val_indices in kf.split(X):\n",
    "    \n",
    "    iteration += 1\n",
    "    X_train = X[train_indices,:]    # Get the training set    \n",
    "    X_val = X[val_indices,:]    # Get the validation set\n",
    "    \n",
    "    print('Iteration {}:'.format(iteration))\n",
    "    print('Indices for validation set:', val_indices)\n",
    "    print('Indices for training set:', train_indices)\n",
    "    print('X_val shape: {}, X_train shape: {} \\n'.format(X_val.shape, X_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Indices for validation set: [0 1 2 3]\n",
      "Indices for training set: [ 4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "X:\n",
      " [[7.147]\n",
      " [6.43 ]\n",
      " [6.012]\n",
      " [6.172]\n",
      " [5.631]\n",
      " [6.004]\n",
      " [6.377]\n",
      " [6.009]\n",
      " [5.889]\n",
      " [5.949]\n",
      " [6.096]\n",
      " [5.834]\n",
      " [5.935]\n",
      " [5.99 ]\n",
      " [5.456]\n",
      " [5.727]]\n",
      "pred_train:\n",
      "  [24.44322792 23.91677767 23.60986525 23.72734369 23.33011972 23.60399133\n",
      " 23.87786294 23.60766253 23.5195537  23.56360812 23.67154143 23.47917049\n",
      " 23.55332876 23.59371197 23.20162768 23.40060679]\n",
      "err_train:\n",
      " [40.31024982  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [24.02324251 23.91016951 24.47112905 24.33382613]\n",
      "err_val:\n",
      " [24.45225404  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[  7.147  54.2  ]\n",
      " [  6.43   58.7  ]\n",
      " [  6.012  66.6  ]\n",
      " [  6.172  96.1  ]\n",
      " [  5.631 100.   ]\n",
      " [  6.004  85.9  ]\n",
      " [  6.377  94.3  ]\n",
      " [  6.009  82.9  ]\n",
      " [  5.889  39.   ]\n",
      " [  5.949  61.8  ]\n",
      " [  6.096  84.5  ]\n",
      " [  5.834  56.5  ]\n",
      " [  5.935  29.3  ]\n",
      " [  5.99   81.7  ]\n",
      " [  5.456  36.6  ]\n",
      " [  5.727  69.5  ]]\n",
      "pred_train:\n",
      "  [21.48063157 22.11691173 23.31494689 27.95444073 28.51363059 26.33941952\n",
      " 27.69204672 25.8696562  18.97684013 22.55648428 26.12883659 21.71463695\n",
      " 17.46081336 25.67972739 18.55892175 23.7420556 ]\n",
      "err_train:\n",
      " [40.31024982 29.18951736  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [23.14974779 25.28236154 22.56585684 20.14958819]\n",
      "err_val:\n",
      " [24.45225404 17.49144968  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00]]\n",
      "pred_train:\n",
      "  [21.01146831 21.77357082 23.16929978 27.20566022 29.22384516 26.74253461\n",
      " 28.60287419 26.24232633 18.97366448 24.59962446 25.0942452  21.7423898\n",
      " 18.68379451 25.29603682 17.40044698 22.33821833]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [19.44922026 24.64092359 21.30971772 16.64580765]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00]]\n",
      "pred_train:\n",
      "  [21.16673125 20.20564757 24.28851665 29.24691712 27.60669625 28.35767511\n",
      " 28.37728135 28.50419678 18.60241761 24.54839834 22.677885   19.14347303\n",
      " 20.48911989 21.97736986 16.29666416 26.61101003]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084 24.34384868  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [18.78886544 24.47317732 21.41512867 16.8771208 ]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243 17.60940755  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01]]\n",
      "pred_train:\n",
      "  [20.93232832 20.58245055 24.25215964 29.36564343 27.73973718 28.57397874\n",
      " 28.62407667 28.04733871 19.03057742 24.43140399 22.73100267 18.51619256\n",
      " 20.55114372 21.8332165  16.38558715 26.50316274]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084 24.34384868 24.26985283  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [18.63735706 23.46858353 20.65343135 15.95593099]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243 17.60940755 15.19515427  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01]]\n",
      "pred_train:\n",
      "  [20.67106952 22.98813668 23.5826845  31.26183597 29.98054739 25.45682935\n",
      " 29.63847662 28.72679987 19.90039786 22.96346078 19.10863124 20.51596022\n",
      " 19.9503173  20.43819669 16.18669588 26.72996012]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084 24.34384868 24.26985283 21.23497731\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [15.06658573 18.87562596 20.96288951 12.63079738]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243 17.60940755 15.19515427 24.50652743\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01]]\n",
      "pred_train:\n",
      "  [20.48937054 22.96613141 23.73208685 31.49149644 29.71611363 25.54379435\n",
      " 29.68443711 28.62558744 19.98550249 22.89897882 19.105674   20.73870682\n",
      " 19.9953526  20.46916287 16.04584623 26.6117584 ]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084 24.34384868 24.26985283 21.23497731\n",
      " 21.21646287  0.          0.          0.        ]\n",
      "pred_val:\n",
      " [14.52007237 18.43397674 20.62095652 11.91005618]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243 17.60940755 15.19515427 24.50652743\n",
      " 28.5570171   0.          0.          0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01]]\n",
      "pred_train:\n",
      "  [20.16981849 23.4363426  24.30610305 31.20955351 29.76723066 25.07616615\n",
      " 29.55963565 28.85186978 19.80027527 22.82088658 19.6039968  20.61923938\n",
      " 20.25680215 20.28501231 15.65056845 26.68649917]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084 24.34384868 24.26985283 21.23497731\n",
      " 21.21646287 21.11725223  0.          0.        ]\n",
      "pred_val:\n",
      " [14.56833317 17.52379044 19.43903388 10.28340792]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243 17.60940755 15.19515427 24.50652743\n",
      " 28.5570171  34.01035113  0.          0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02]]\n",
      "pred_train:\n",
      "  [20.16918697 23.52183554 25.27174257 31.24953774 29.44524961 24.71505902\n",
      " 29.91264092 28.38884348 19.42839034 23.76334934 19.24586584 20.45344882\n",
      " 19.36678267 20.13383873 16.22686844 26.80735997]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084 24.34384868 24.26985283 21.23497731\n",
      " 21.21646287 21.11725223 20.87609973  0.        ]\n",
      "pred_val:\n",
      " [14.41821151 17.83769219 21.03003721 14.35052499]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243 17.60940755 15.19515427 24.50652743\n",
      " 28.5570171  34.01035113 22.74872672  0.        ]\n",
      "X:\n",
      " [[ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00  7.42044161e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01  2.10025514e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01  5.86623191e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01 -2.98092835e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00 -5.04465863e-01]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01  7.62011180e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01  3.77563786e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01  7.73400683e-02]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01  5.93578523e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00  3.15159392e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00  4.08900538e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00  3.38011697e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01 -6.23530730e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01  7.50411640e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01 -2.43483776e+00]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02 -1.12201873e+00]]\n",
      "pred_train:\n",
      "  [20.2765257  22.9275231  25.24860277 31.7157062  29.55560181 24.15046133\n",
      " 30.45106052 28.17941689 18.9826411  23.5728683  19.62867916 20.34595016\n",
      " 19.98638864 19.8540764  16.706406   26.51809193]\n",
      "err_train:\n",
      " [40.31024982 29.18951736 28.39223084 24.34384868 24.26985283 21.23497731\n",
      " 21.21646287 21.11725223 20.87609973 20.72508917]\n",
      "pred_val:\n",
      " [14.27891426 17.55862304 20.14054212 12.29468758]\n",
      "err_val:\n",
      " [24.45225404 17.49144968 16.99199243 17.60940755 15.19515427 24.50652743\n",
      " 28.5570171  34.01035113 22.74872672 28.17281011]\n",
      "Iteration 2:\n",
      "Indices for validation set: [4 5 6 7]\n",
      "Indices for training set: [ 0  1  2  3  8  9 10 11 12 13 14 15 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "X:\n",
      " [[6.575]\n",
      " [6.421]\n",
      " [7.185]\n",
      " [6.998]\n",
      " [5.631]\n",
      " [6.004]\n",
      " [6.377]\n",
      " [6.009]\n",
      " [5.889]\n",
      " [5.949]\n",
      " [6.096]\n",
      " [5.834]\n",
      " [5.935]\n",
      " [5.99 ]\n",
      " [5.456]\n",
      " [5.727]]\n",
      "pred_train:\n",
      "  [26.62495453 25.58951265 30.72638016 29.46905788 20.27783027 22.78575119\n",
      " 25.29367211 22.81936943 22.01253161 22.41595052 23.40432686 21.64273093\n",
      " 22.32181944 22.69162011 19.10119177 20.92330054]\n",
      "err_train:\n",
      " [30.93740741  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [30.47088151 25.65002549 22.83954038 23.91532415]\n",
      "err_val:\n",
      " [40.09622833  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[  6.575  65.2  ]\n",
      " [  6.421  78.9  ]\n",
      " [  7.185  61.1  ]\n",
      " [  6.998  45.8  ]\n",
      " [  5.631 100.   ]\n",
      " [  6.004  85.9  ]\n",
      " [  6.377  94.3  ]\n",
      " [  6.009  82.9  ]\n",
      " [  5.889  39.   ]\n",
      " [  5.949  61.8  ]\n",
      " [  6.096  84.5  ]\n",
      " [  5.834  56.5  ]\n",
      " [  5.935  29.3  ]\n",
      " [  5.99   81.7  ]\n",
      " [  5.456  36.6  ]\n",
      " [  5.727  69.5  ]]\n",
      "pred_train:\n",
      "  [26.48428754 26.56073653 30.26690317 27.75895637 22.94785714 24.31672794\n",
      " 27.51795163 24.10615611 19.72122656 21.98291356 24.82356571 20.77522909\n",
      " 19.24170429 23.88021142 16.60372758 21.11184535]\n",
      "err_train:\n",
      " [30.93740741 28.09041146  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [29.44854323 24.97642623 22.79896337 26.28112221]\n",
      "err_val:\n",
      " [40.09622833 42.06502427  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00]]\n",
      "pred_train:\n",
      "  [26.18246181 26.5789338  30.39548798 27.59961361 22.93503331 24.36583717\n",
      " 27.69163149 24.15749796 19.7742432  22.26093032 24.71800072 20.79200391\n",
      " 19.47000741 23.83715559 16.43085133 20.91031041]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [29.67692694 25.06612638 22.81434144 26.20697909]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00]]\n",
      "pred_train:\n",
      "  [26.01337586 26.61555972 30.35753581 27.7456779  22.2505268  25.61772116\n",
      " 27.69588604 25.80690917 19.39916786 22.09793398 23.43749972 19.12155812\n",
      " 20.31683682 21.9119822  15.74583589 23.96599295]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527 26.58910148  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [29.55439787 23.99514607 23.62725352 27.8854464 ]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291 47.35002823  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01]]\n",
      "pred_train:\n",
      "  [25.46381027 27.61007965 30.37985148 28.01260439 22.23011218 25.17901743\n",
      " 26.96656726 26.35748167 18.64642461 22.23736777 23.08577724 19.98578267\n",
      " 20.12790798 21.95524593 15.7509489  24.11102057]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527 26.58910148 26.34497425  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [28.85485627 22.92406762 23.50382028 27.42522424]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291 47.35002823 39.63971772  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01]]\n",
      "pred_train:\n",
      "  [24.6618525  23.90596573 33.48343228 27.7476815  25.05784252 22.07647916\n",
      " 28.97871762 27.85393189 19.81788276 19.83527948 20.31080511 22.75321205\n",
      " 18.83499917 21.31954266 15.85857954 25.60379603]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527 26.58910148 26.34497425 21.67040664\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [30.62631994 26.95133796 23.4528113  31.27850086]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291 47.35002823 39.63971772 81.90031184\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01]]\n",
      "pred_train:\n",
      "  [26.0726411  25.61126883 32.71353531 29.99177094 31.66206592 19.16157109\n",
      " 28.24580219 27.36896852 19.34118275 22.8147278  17.36495049 17.77029133\n",
      " 19.15992183 19.26209077 17.87134934 23.68786179]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527 26.58910148 26.34497425 21.67040664\n",
      " 14.29012445  0.          0.          0.        ]\n",
      "pred_val:\n",
      " [29.47917853 27.07660509 18.86139149 24.11500228]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291 47.35002823 39.63971772 81.90031184\n",
      " 42.29610211  0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01]]\n",
      "pred_train:\n",
      "  [23.32384763 26.25436393 33.16574513 31.70343519 31.8186136  22.18913882\n",
      " 28.69593981 25.46081356 20.27977511 22.13249715 15.31618795 17.45537233\n",
      " 17.02917364 19.93278852 19.61445146 23.72785618]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527 26.58910148 26.34497425 21.67040664\n",
      " 14.29012445 11.92693683  0.          0.        ]\n",
      "pred_val:\n",
      " [27.14023251 23.96033547 16.4507473  26.50845532]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291 47.35002823 39.63971772 81.90031184\n",
      " 42.29610211 35.28668471  0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02]]\n",
      "pred_train:\n",
      "  [24.01625122 27.36754174 33.71228123 30.43723721 32.11865312 22.64710969\n",
      " 28.20863222 25.44467341 20.87614771 20.93038329 14.55745638 17.4143267\n",
      " 17.51805504 20.00110864 19.57387248 23.27626995]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527 26.58910148 26.34497425 21.67040664\n",
      " 14.29012445 11.92693683 11.49035042  0.        ]\n",
      "pred_val:\n",
      " [28.04785589 23.36445098 14.00561722 26.30302035]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291 47.35002823 39.63971772 81.90031184\n",
      " 42.29610211 35.28668471 40.725595    0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00 -7.61206901e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00 -1.09989127e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01  5.02494339e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01 -3.96753527e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00 -5.04465863e-01]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01  7.62011180e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01  3.77563786e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01  7.73400683e-02]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01  5.93578523e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00  3.15159392e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00  4.08900538e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00  3.38011697e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01 -6.23530730e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01  7.50411640e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01 -2.43483776e+00]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02 -1.12201873e+00]]\n",
      "pred_train:\n",
      "  [24.26283074 26.29663685 33.24044957 31.43023702 32.93382074 23.54838438\n",
      " 25.69361574 26.39587584 22.9583951  21.67081491 14.09762511 17.6362475\n",
      " 16.87935106 20.97266466 17.07710315 23.00594763]\n",
      "err_train:\n",
      " [30.93740741 28.09041146 28.06636527 26.58910148 26.34497425 21.67040664\n",
      " 14.29012445 11.92693683 11.49035042  9.99493275]\n",
      "pred_val:\n",
      " [27.06252317 26.89254996 13.66848304 22.37498068]\n",
      "err_val:\n",
      " [40.09622833 42.06502427 42.86218291 47.35002823 39.63971772 81.90031184\n",
      " 42.29610211 35.28668471 40.725595   41.00319054]\n",
      "Iteration 3:\n",
      "Indices for validation set: [ 8  9 10 11]\n",
      "Indices for training set: [ 0  1  2  3  4  5  6  7 12 13 14 15 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "X:\n",
      " [[6.575]\n",
      " [6.421]\n",
      " [7.185]\n",
      " [6.998]\n",
      " [7.147]\n",
      " [6.43 ]\n",
      " [6.012]\n",
      " [6.172]\n",
      " [5.889]\n",
      " [5.949]\n",
      " [6.096]\n",
      " [5.834]\n",
      " [5.935]\n",
      " [5.99 ]\n",
      " [5.456]\n",
      " [5.727]]\n",
      "pred_train:\n",
      "  [27.41999563 25.68606746 34.28815263 32.18266843 33.86030023 25.78740093\n",
      " 21.08102449 22.88250829 19.69613382 20.37169024 22.02680349 19.07687376\n",
      " 20.21406041 20.83332047 14.82086827 17.87213146]\n",
      "err_train:\n",
      " [9.05075804 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "pred_val:\n",
      " [16.79124118 20.9909503  25.19065942 21.04724667]\n",
      "err_val:\n",
      " [21.25016671  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.575 65.2  ]\n",
      " [ 6.421 78.9  ]\n",
      " [ 7.185 61.1  ]\n",
      " [ 6.998 45.8  ]\n",
      " [ 7.147 54.2  ]\n",
      " [ 6.43  58.7  ]\n",
      " [ 6.012 66.6  ]\n",
      " [ 6.172 96.1  ]\n",
      " [ 5.889 39.   ]\n",
      " [ 5.949 61.8  ]\n",
      " [ 6.096 84.5  ]\n",
      " [ 5.834 56.5  ]\n",
      " [ 5.935 29.3  ]\n",
      " [ 5.99  81.7  ]\n",
      " [ 5.456 36.6  ]\n",
      " [ 5.727 69.5  ]]\n",
      "pred_train:\n",
      "  [27.31161083 25.05591699 34.37922976 32.82887438 34.20528835 25.90981997\n",
      " 20.87728633 21.59301943 20.51026704 20.34177731 21.16324887 19.23540287\n",
      " 21.39277725 20.06584493 15.69107563 17.53856006]\n",
      "err_train:\n",
      " [9.05075804 8.616102   0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "pred_val:\n",
      " [15.3150345  20.06821896 23.98391528 20.23656445]\n",
      "err_val:\n",
      " [21.25016671 21.41676154  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00]]\n",
      "pred_train:\n",
      "  [26.9815649  25.10857666 34.45368007 32.5719101  34.3694603  25.98488058\n",
      " 20.92097069 21.60873034 20.52981087 20.65389829 21.12054739 19.26563001\n",
      " 21.56904203 20.08849728 15.49366622 17.37913427]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "pred_val:\n",
      " [15.43978962 20.19805203 24.24324748 20.36065029]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00]]\n",
      "pred_train:\n",
      "  [26.61717406 25.17003866 34.63553626 32.74470603 34.57062648 25.03502627\n",
      " 21.80308452 23.21321591 20.30808503 20.76369701 19.68015514 17.61086403\n",
      " 22.7938211  18.03942174 14.74630935 20.36823838]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 7.05513431 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "pred_val:\n",
      " [14.61318005 21.49317992 24.3457041  22.07901837]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191 30.05131119  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01]]\n",
      "pred_train:\n",
      "  [26.93429994 24.35654402 34.405764   32.33387083 34.86739059 25.73267482\n",
      " 21.84782386 23.55924136 20.78474319 20.60030832 19.95553818 16.93359768\n",
      " 22.79483431 18.026633   14.71938844 20.24734746]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 7.05513431 6.89986123 0.\n",
      " 0.         0.         0.         0.        ]\n",
      "pred_val:\n",
      " [14.72588688 21.81676354 24.86379848 21.64613574]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191 30.05131119 30.59677146  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01]]\n",
      "pred_train:\n",
      "  [25.20467806 22.15770046 36.43096056 31.80979138 34.81987803 27.72788756\n",
      " 21.37458289 25.82116719 20.81289145 18.64581076 17.26896057 20.77789799\n",
      " 21.11371083 17.88394832 15.10396286 21.14617109]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 7.05513431 6.89986123 3.70326775\n",
      " 0.         0.         0.         0.        ]\n",
      "pred_val:\n",
      " [17.44279015 18.23615256 25.41497393 23.23823292]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191 30.05131119 30.59677146 21.28142201\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01]]\n",
      "pred_train:\n",
      "  [25.29264897 22.36456826 36.52217407 31.77822012 34.63361475 28.14913869\n",
      " 20.92696828 25.71895584 20.73666975 19.10410248 16.94334113 20.48775894\n",
      " 20.84159914 17.9261547  15.33390644 21.34017846]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 7.05513431 6.89986123 3.70326775\n",
      " 3.63731831 0.         0.         0.        ]\n",
      "pred_val:\n",
      " [19.48329628 17.98667253 26.04099648 23.82273167]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191 30.05131119 30.59677146 21.28142201\n",
      " 19.76243681  0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01]]\n",
      "pred_train:\n",
      "  [25.06810275 22.37724421 36.58156001 31.95545047 34.60572018 27.9157082\n",
      " 20.76452393 25.98878676 21.00652328 19.10910551 16.91885785 20.31209841\n",
      " 20.78078428 18.05341315 15.47453018 21.18759082]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 7.05513431 6.89986123 3.70326775\n",
      " 3.63731831 3.611866   0.         0.        ]\n",
      "pred_val:\n",
      " [19.19225573 18.55656953 26.18417058 23.41710419]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191 30.05131119 30.59677146 21.28142201\n",
      " 19.76243681 19.8538195   0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02]]\n",
      "pred_train:\n",
      "  [24.36589699 21.51052082 36.45283907 33.22630208 34.01160151 28.34940659\n",
      " 22.06613229 25.85156665 20.55697712 19.97822236 17.11359387 20.08764548\n",
      " 20.13455266 17.92005649 15.47529487 20.99939116]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 7.05513431 6.89986123 3.70326775\n",
      " 3.63731831 3.611866   3.19645373 0.        ]\n",
      "pred_val:\n",
      " [19.08284559 18.02961096 26.59814637 23.02529256]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191 30.05131119 30.59677146 21.28142201\n",
      " 19.76243681 19.8538195  20.15943564  0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00 -7.61206901e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00 -1.09989127e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01  5.02494339e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01 -3.96753527e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00  7.42044161e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01  2.10025514e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01  5.86623191e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01 -2.98092835e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01  5.93578523e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00  3.15159392e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00  4.08900538e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00  3.38011697e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01 -6.23530730e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01  7.50411640e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01 -2.43483776e+00]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02 -1.12201873e+00]]\n",
      "pred_train:\n",
      "  [24.64449218 21.23269641 36.78202463 32.48802709 34.98833311 27.79470492\n",
      " 22.26702607 26.89975126 18.38619026 20.34845324 17.3425252  19.88651393\n",
      " 20.27326852 16.92802734 17.93718305 19.90078279]\n",
      "err_train:\n",
      " [9.05075804 8.616102   8.59019409 7.05513431 6.89986123 3.70326775\n",
      " 3.63731831 3.611866   3.19645373 2.17005308]\n",
      "pred_val:\n",
      " [21.44721908 14.05259177 29.2368164  22.3426814 ]\n",
      "err_val:\n",
      " [21.25016671 21.41676154 21.74364191 30.05131119 30.59677146 21.28142201\n",
      " 19.76243681 19.8538195  20.15943564 28.36054206]\n",
      "Iteration 4:\n",
      "Indices for validation set: [12 13 14 15]\n",
      "Indices for training set: [ 0  1  2  3  4  5  6  7  8  9 10 11 16 17 18 19]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "X:\n",
      " [[6.575]\n",
      " [6.421]\n",
      " [7.185]\n",
      " [6.998]\n",
      " [7.147]\n",
      " [6.43 ]\n",
      " [6.012]\n",
      " [6.172]\n",
      " [5.631]\n",
      " [6.004]\n",
      " [6.377]\n",
      " [6.009]\n",
      " [5.935]\n",
      " [5.99 ]\n",
      " [5.456]\n",
      " [5.727]]\n",
      "pred_train:\n",
      "  [27.01885395 25.3920646  33.46262992 31.48724286 33.06121437 25.48713671\n",
      " 21.07156563 22.76173638 17.04684654 20.98705709 24.92726765 21.03987493\n",
      " 20.25817096 20.83916715 15.19822229 18.06094899]\n",
      "err_train:\n",
      " [12.0918875  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.       ]\n",
      "pred_val:\n",
      " [19.77224687 20.4060609  21.95890527 19.19125067]\n",
      "err_val:\n",
      " [5.89886413 0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "X:\n",
      " [[  6.575  65.2  ]\n",
      " [  6.421  78.9  ]\n",
      " [  7.185  61.1  ]\n",
      " [  6.998  45.8  ]\n",
      " [  7.147  54.2  ]\n",
      " [  6.43   58.7  ]\n",
      " [  6.012  66.6  ]\n",
      " [  6.172  96.1  ]\n",
      " [  5.631 100.   ]\n",
      " [  6.004  85.9  ]\n",
      " [  6.377  94.3  ]\n",
      " [  6.009  82.9  ]\n",
      " [  5.935  29.3  ]\n",
      " [  5.99   81.7  ]\n",
      " [  5.456  36.6  ]\n",
      " [  5.727  69.5  ]]\n",
      "pred_train:\n",
      "  [27.1547504  24.47400563 33.5668078  32.98945179 33.76735474 26.25732292\n",
      " 21.43664523 20.55472485 14.84584839 19.73874161 22.74499262 20.04003382\n",
      " 23.79828412 19.95164307 18.42108997 18.35830303]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [22.52732451 21.21240602 20.77134706 20.51279709]\n",
      "err_val:\n",
      " [5.89886413 4.94634588 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "X:\n",
      " [[  6.575       65.2          1.62434536]\n",
      " [  6.421       78.9          0.3190391 ]\n",
      " [  7.185       61.1         -0.17242821]\n",
      " [  6.998       45.8          0.90085595]\n",
      " [  7.147       54.2         -0.6871727 ]\n",
      " [  6.43        58.7         -0.19183555]\n",
      " [  6.012       66.6          0.12015895]\n",
      " [  6.172       96.1          0.83898341]\n",
      " [  5.631      100.           0.48851815]\n",
      " [  6.004       85.9          0.16003707]\n",
      " [  6.377       94.3         -0.22232814]\n",
      " [  6.009       82.9          0.12182127]\n",
      " [  5.935       29.3         -1.19926803]\n",
      " [  5.99        81.7          0.52057634]\n",
      " [  5.456       36.6          0.52946532]\n",
      " [  5.727       69.5          1.0388246 ]]\n",
      "pred_train:\n",
      "  [28.08107033 24.46914097 33.41083203 33.59609531 33.3163196  26.03975613\n",
      " 21.33999768 20.76556798 14.76384468 19.55687239 22.30007835 19.85111684\n",
      " 23.0590314  20.024649   18.71296255 18.81266476]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [22.27941747 20.3558761  21.04673538 20.36584167]\n",
      "err_val:\n",
      " [5.89886413 4.94634588 3.55930373 0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00]]\n",
      "pred_train:\n",
      "  [27.9157105  24.38548412 33.44197084 33.64660443 33.34070014 25.61711088\n",
      " 21.49497207 21.17830784 14.29343946 19.83729468 22.19978217 20.2753567\n",
      " 23.31116465 19.19267522 18.29807623 19.67135005]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  8.9404243   0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [22.05783511 20.21445975 20.43694055 19.6536477 ]\n",
      "err_val:\n",
      " [5.89886413 4.94634588 3.55930373 2.65590797 0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01]]\n",
      "pred_train:\n",
      "  [28.44928814 23.2187251  33.0990346  33.27257687 33.64904745 26.38047991\n",
      " 21.52422757 21.62946057 14.16772554 20.18116698 22.63813216 19.66614094\n",
      " 23.29200442 18.91567173 18.27803184 19.73828616]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  8.9404243   8.7176077   0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [22.62744828 19.76565572 20.6201144  18.56515274]\n",
      "err_val:\n",
      " [5.89886413 4.94634588 3.55930373 2.65590797 1.4165834  0.\n",
      " 0.         0.         0.         0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01]]\n",
      "pred_train:\n",
      "  [26.79644197 21.40780951 34.9904476  33.35261509 33.57032239 27.76451546\n",
      " 21.06850339 22.99754899 15.78768693 16.57336516 22.29673871 21.01516794\n",
      " 22.3969337  18.33624772 19.03893353 20.70672192]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  8.9404243   8.7176077   6.60342503\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [22.78102174 18.07423805 17.69421571 22.21730112]\n",
      "err_val:\n",
      " [5.89886413 4.94634588 3.55930373 2.65590797 1.4165834  5.71228993\n",
      " 0.         0.         0.         0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01]]\n",
      "pred_train:\n",
      "  [25.82338825 21.23257603 35.43262061 32.69725085 33.40861225 27.6401628\n",
      " 22.75231746 24.94812991 13.19224219 16.37029375 21.63344583 20.84185255\n",
      " 21.90603021 20.54346385 19.36030778 20.31730568]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  8.9404243   8.7176077   6.60342503\n",
      "  5.29720929  0.          0.          0.        ]\n",
      "pred_val:\n",
      " [23.04866373 16.79589199 19.31177094 27.08066899]\n",
      "err_val:\n",
      " [ 5.89886413  4.94634588  3.55930373  2.65590797  1.4165834   5.71228993\n",
      " 20.03840902  0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01]]\n",
      "pred_train:\n",
      "  [25.30651587 21.13418852 35.6874767  33.09148605 33.29315604 27.3882625\n",
      " 22.26322645 25.26190073 13.24133759 16.83497845 21.81255344 20.41858972\n",
      " 21.83607501 20.4809786  19.78594543 20.26332887]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  8.9404243   8.7176077   6.60342503\n",
      "  5.29720929  5.20132751  0.          0.        ]\n",
      "pred_val:\n",
      " [23.5118994  16.81712713 18.715426   26.87672469]\n",
      "err_val:\n",
      " [ 5.89886413  4.94634588  3.55930373  2.65590797  1.4165834   5.71228993\n",
      " 20.03840902 19.53137196  0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02]]\n",
      "pred_train:\n",
      "  [25.6641735  21.34459449 35.91202933 32.39437245 33.62584851 27.26506749\n",
      " 21.44100051 25.55182983 13.15939268 16.84561262 21.44106862 20.50775345\n",
      " 22.02511415 20.62611916 19.9452315  20.35079173]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  8.9404243   8.7176077   6.60342503\n",
      "  5.29720929  5.20132751  5.08650506  0.        ]\n",
      "pred_val:\n",
      " [23.89634729 15.91542928 18.46350503 27.44354965]\n",
      "err_val:\n",
      " [ 5.89886413  4.94634588  3.55930373  2.65590797  1.4165834   5.71228993\n",
      " 20.03840902 19.53137196 22.90091455  0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00 -7.61206901e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00 -1.09989127e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01  5.02494339e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01 -3.96753527e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00  7.42044161e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01  2.10025514e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01  5.86623191e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01 -2.98092835e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00 -5.04465863e-01]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01  7.62011180e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01  3.77563786e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01  7.73400683e-02]\n",
      " [ 5.93500000e+00  2.93000000e+01 -1.19926803e+00  8.63345318e-01\n",
      "  -1.80920302e-01 -6.03920628e-01 -1.23005814e+00  5.50537496e-01\n",
      "   7.92806866e-01 -6.23530730e-01]\n",
      " [ 5.99000000e+00  8.17000000e+01  5.20576337e-01 -1.14434139e+00\n",
      "   8.01861032e-01  4.65672984e-02 -1.86569772e-01 -1.01745873e-01\n",
      "   8.68886157e-01  7.50411640e-01]\n",
      " [ 5.45600000e+00  3.66000000e+01  5.29465324e-01  1.37701210e-01\n",
      "   7.78211279e-02  6.18380262e-01  2.32494559e-01  6.82551407e-01\n",
      "  -3.10116774e-01 -2.43483776e+00]\n",
      " [ 5.72700000e+00  6.95000000e+01  1.03882460e+00  2.18697965e+00\n",
      "   4.41364444e-01 -1.00155233e-01 -1.36444744e-01 -1.19054188e-01\n",
      "   1.74094083e-02 -1.12201873e+00]]\n",
      "pred_train:\n",
      "  [25.04360854 20.71443131 36.29938771 32.86771159 33.03003004 28.97858315\n",
      " 20.74495143 24.72198157 13.13437615 18.37864491 19.778848   21.22300459\n",
      " 21.76708616 21.5414279  18.94544702 20.93047992]\n",
      "err_train:\n",
      " [12.0918875   9.26783277  9.10083911  8.9404243   8.7176077   6.60342503\n",
      "  5.29720929  5.20132751  5.08650506  4.24363492]\n",
      "pred_val:\n",
      " [26.17404315 15.53858004 17.3698992  28.83661038]\n",
      "err_val:\n",
      " [ 5.89886413  4.94634588  3.55930373  2.65590797  1.4165834   5.71228993\n",
      " 20.03840902 19.53137196 22.90091455 33.6109651 ]\n",
      "Iteration 5:\n",
      "Indices for validation set: [16 17 18 19]\n",
      "Indices for training set: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "X_val shape: (4, 10), X_train shape: (16, 10) \n",
      "\n",
      "X:\n",
      " [[6.575]\n",
      " [6.421]\n",
      " [7.185]\n",
      " [6.998]\n",
      " [7.147]\n",
      " [6.43 ]\n",
      " [6.012]\n",
      " [6.172]\n",
      " [5.631]\n",
      " [6.004]\n",
      " [6.377]\n",
      " [6.009]\n",
      " [5.889]\n",
      " [5.949]\n",
      " [6.096]\n",
      " [5.834]]\n",
      "pred_train:\n",
      "  [26.88095905 25.0900212  33.97493361 31.80022338 33.53301388 25.1946864\n",
      " 20.33356941 22.19428405 15.90274267 20.24053367 24.57832468 20.29868101\n",
      " 18.90314503 19.60091302 21.31044459 18.26352437]\n",
      "err_train:\n",
      " [11.89287799  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [19.43810048 20.07772114 13.86758603 17.01917145]\n",
      "err_val:\n",
      " [15.38699427  0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[  6.575  65.2  ]\n",
      " [  6.421  78.9  ]\n",
      " [  7.185  61.1  ]\n",
      " [  6.998  45.8  ]\n",
      " [  7.147  54.2  ]\n",
      " [  6.43   58.7  ]\n",
      " [  6.012  66.6  ]\n",
      " [  6.172  96.1  ]\n",
      " [  5.631 100.   ]\n",
      " [  6.004  85.9  ]\n",
      " [  6.377  94.3  ]\n",
      " [  6.009  82.9  ]\n",
      " [  5.889  39.   ]\n",
      " [  5.949  61.8  ]\n",
      " [  6.096  84.5  ]\n",
      " [  5.834  56.5  ]]\n",
      "pred_train:\n",
      "  [27.01848761 24.09228038 33.62271982 33.24391957 33.92256564 26.19161309\n",
      " 21.16008855 19.85311081 13.96969558 19.16037551 22.11452628 19.50937192\n",
      " 22.65406845 20.99722908 20.23411961 20.35582809]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [24.08555216 19.43564191 18.49399972 17.97665141]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02]]\n",
      "pred_train:\n",
      "  [27.133368   24.0938239  33.58911287 33.31323701 33.85596596 26.17270654\n",
      " 21.16561507 19.88317966 13.9782902  19.15017393 22.05912643 19.49902783\n",
      " 22.65254395 20.91306892 20.27757511 20.36318461]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [24.02545527 19.45819294 18.57148092 18.05681211]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01]]\n",
      "pred_train:\n",
      "  [27.02372686 23.9155198  33.41637719 33.74181479 33.74466114 25.32004697\n",
      " 22.14796836 21.05214014 12.94891545 20.15932081 21.52598294 20.95609695\n",
      " 23.08761453 21.03184721 18.83167496 19.1962919 ]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  8.44343841  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [25.81453304 17.47169541 18.93960663 21.29814137]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  4.63914054  0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00]]\n",
      "pred_train:\n",
      "  [27.27619009 23.23385964 33.205049   33.47098951 33.94575592 25.82541646\n",
      " 22.18472221 21.26450527 12.92496305 20.36352205 21.81033119 20.5945318\n",
      " 23.50568843 20.87446291 18.9639131  18.65609938]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  8.44343841  8.33397609  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [25.89957164 17.37168466 19.00230914 21.27506102]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  4.63914054  4.68613246  0.\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00]]\n",
      "pred_train:\n",
      "  [26.51263113 21.29004794 34.48874201 33.39137948 33.82108631 27.07740971\n",
      " 22.01718545 23.34494366 14.38675496 18.04981483 21.95380687 21.82570114\n",
      " 23.60039962 18.97682404 16.74665916 20.61661367]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  8.44343841  8.33397609  6.2495243\n",
      "  0.          0.          0.          0.        ]\n",
      "pred_val:\n",
      " [24.81531151 16.59444407 19.34899377 22.91667813]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  4.63914054  4.68613246  6.68339731\n",
      "  0.          0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00]]\n",
      "pred_train:\n",
      "  [26.30231632 21.2328513  34.53243478 33.12247787 33.90533192 26.99318682\n",
      " 22.28087135 23.71442783 13.71460804 18.2031867  21.9753876  21.64945754\n",
      " 23.48667614 18.72166541 17.14114068 21.12397972]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  8.44343841  8.33397609  6.2495243\n",
      "  6.16578259  0.          0.          0.        ]\n",
      "pred_val:\n",
      " [24.59590454 16.93936035 19.00926812 22.61735725]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  4.63914054  4.68613246  6.68339731\n",
      "  5.87073368  0.          0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00]]\n",
      "pred_train:\n",
      "  [26.17116824 21.25614066 34.59874188 33.26842412 33.81036231 26.89432563\n",
      " 22.10193762 23.8225483  13.76216142 18.37145839 22.03621987 21.44301908\n",
      " 23.62607459 18.69710911 17.07346282 21.16684595]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  8.44343841  8.33397609  6.2495243\n",
      "  6.16578259  6.15270221  0.          0.        ]\n",
      "pred_val:\n",
      " [24.45896964 17.07362233 19.19303087 22.50352126]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  4.63914054  4.68613246  6.68339731\n",
      "  5.87073368  5.39071961  0.          0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00]]\n",
      "pred_train:\n",
      "  [25.98566641 21.06731577 34.5712938  33.60056524 33.61283752 26.95708574\n",
      " 22.40103816 23.83316252 13.76611635 18.25491577 22.17338039 21.32293215\n",
      " 23.42824441 18.87045811 17.15736496 21.0976227 ]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  8.44343841  8.33397609  6.2495243\n",
      "  6.16578259  6.15270221  6.1251075   0.        ]\n",
      "pred_val:\n",
      " [24.15083476 17.08628364 19.12686029 22.41118033]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  4.63914054  4.68613246  6.68339731\n",
      "  5.87073368  5.39071961  5.04027088  0.        ]\n",
      "X:\n",
      " [[ 6.57500000e+00  6.52000000e+01  1.62434536e+00 -6.11756414e-01\n",
      "  -5.28171752e-01 -1.07296862e+00  8.65407629e-01 -2.30153870e+00\n",
      "   1.74481176e+00 -7.61206901e-01]\n",
      " [ 6.42100000e+00  7.89000000e+01  3.19039096e-01 -2.49370375e-01\n",
      "   1.46210794e+00 -2.06014071e+00 -3.22417204e-01 -3.84054355e-01\n",
      "   1.13376944e+00 -1.09989127e+00]\n",
      " [ 7.18500000e+00  6.11000000e+01 -1.72428208e-01 -8.77858418e-01\n",
      "   4.22137467e-02  5.82815214e-01 -1.10061918e+00  1.14472371e+00\n",
      "   9.01590721e-01  5.02494339e-01]\n",
      " [ 6.99800000e+00  4.58000000e+01  9.00855949e-01 -6.83727859e-01\n",
      "  -1.22890226e-01 -9.35769434e-01 -2.67888080e-01  5.30355467e-01\n",
      "  -6.91660752e-01 -3.96753527e-01]\n",
      " [ 7.14700000e+00  5.42000000e+01 -6.87172700e-01 -8.45205641e-01\n",
      "  -6.71246131e-01 -1.26645989e-02 -1.11731035e+00  2.34415698e-01\n",
      "   1.65980218e+00  7.42044161e-01]\n",
      " [ 6.43000000e+00  5.87000000e+01 -1.91835552e-01 -8.87628964e-01\n",
      "  -7.47158294e-01  1.69245460e+00  5.08077548e-02 -6.36995647e-01\n",
      "   1.90915485e-01  2.10025514e+00]\n",
      " [ 6.01200000e+00  6.66000000e+01  1.20158952e-01  6.17203110e-01\n",
      "   3.00170320e-01 -3.52249846e-01 -1.14251820e+00 -3.49342722e-01\n",
      "  -2.08894233e-01  5.86623191e-01]\n",
      " [ 6.17200000e+00  9.61000000e+01  8.38983414e-01  9.31102081e-01\n",
      "   2.85587325e-01  8.85141164e-01 -7.54397941e-01  1.25286816e+00\n",
      "   5.12929820e-01 -2.98092835e-01]\n",
      " [ 5.63100000e+00  1.00000000e+02  4.88518147e-01 -7.55717130e-02\n",
      "   1.13162939e+00  1.51981682e+00  2.18557541e+00 -1.39649634e+00\n",
      "  -1.44411381e+00 -5.04465863e-01]\n",
      " [ 6.00400000e+00  8.59000000e+01  1.60037069e-01  8.76168921e-01\n",
      "   3.15634947e-01 -2.02220122e+00 -3.06204013e-01  8.27974643e-01\n",
      "   2.30094735e-01  7.62011180e-01]\n",
      " [ 6.37700000e+00  9.43000000e+01 -2.22328143e-01 -2.00758069e-01\n",
      "   1.86561391e-01  4.10051647e-01  1.98299720e-01  1.19008646e-01\n",
      "  -6.70662286e-01  3.77563786e-01]\n",
      " [ 6.00900000e+00  8.29000000e+01  1.21821271e-01  1.12948391e+00\n",
      "   1.19891788e+00  1.85156417e-01 -3.75284950e-01 -6.38730407e-01\n",
      "   4.23494354e-01  7.73400683e-02]\n",
      " [ 5.88900000e+00  3.90000000e+01 -3.43853676e-01  4.35968568e-02\n",
      "  -6.20000844e-01  6.98032034e-01 -4.47128565e-01  1.22450770e+00\n",
      "   4.03491642e-01  5.93578523e-01]\n",
      " [ 5.94900000e+00  6.18000000e+01 -1.09491185e+00  1.69382433e-01\n",
      "   7.40556451e-01 -9.53700602e-01 -2.66218506e-01  3.26145467e-02\n",
      "  -1.37311732e+00  3.15159392e-01]\n",
      " [ 6.09600000e+00  8.45000000e+01  8.46160648e-01 -8.59515941e-01\n",
      "   3.50545979e-01 -1.31228341e+00 -3.86955093e-02 -1.61577235e+00\n",
      "   1.12141771e+00  4.08900538e-01]\n",
      " [ 5.83400000e+00  5.65000000e+01 -2.46169559e-02 -7.75161619e-01\n",
      "   1.27375593e+00  1.96710175e+00 -1.85798186e+00  1.23616403e+00\n",
      "   1.62765075e+00  3.38011697e-01]]\n",
      "pred_train:\n",
      "  [25.73278092 20.97126703 34.75715322 33.70542449 33.35385076 27.56582504\n",
      " 22.26907261 23.50272972 13.72666662 18.83675829 21.8881933  21.62020536\n",
      " 23.21369605 18.60178331 17.23997438 21.11461889]\n",
      "err_train:\n",
      " [11.89287799  9.16258967  9.16026194  8.44343841  8.33397609  6.2495243\n",
      "  6.16578259  6.15270221  6.1251075   6.04272723]\n",
      "pred_val:\n",
      " [22.91653815 17.58627089 17.29003905 22.02389243]\n",
      "err_val:\n",
      " [15.38699427  1.91958606  1.84089106  4.63914054  4.68613246  6.68339731\n",
      "  5.87073368  5.39071961  5.04027088  5.78278175]\n"
     ]
    }
   ],
   "source": [
    "# Import KFold class from scikitlearn library\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "K = 5    # Specify the number of folds to split data into\n",
    "kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "# For all splits, print the validation and training indices\n",
    "iteration = 0\n",
    "for train_indices, val_indices in kf.split(X):\n",
    "    \n",
    "    iteration += 1\n",
    "    X_train = X[train_indices,:]    # Get the training set    \n",
    "    X_val = X[val_indices,:]    # Get the validation set\n",
    "    \n",
    "    print('Iteration {}:'.format(iteration))\n",
    "    print('Indices for validation set:', val_indices)\n",
    "    print('Indices for training set:', train_indices)\n",
    "    print('X_val shape: {}, X_train shape: {} \\n'.format(X_val.shape, X_train.shape))\n",
    "    err_train = np.zeros(n)  # Array for storing training errors\n",
    "    err_val = np.zeros(n)    # Array for storing validation errors\n",
    "    \n",
    "    for i in range(n):    # Loop over the number of features r \n",
    "        ### STUDENT TASK ###\n",
    "        # YOUR CODE HERE\n",
    "        reg = LinearRegression(fit_intercept=True)\n",
    "        #fit linear model #reg.fit(X[:,:(i+1)], y)\n",
    "        reg.fit(X_train[:,:(i+1)], y_train)\n",
    "        # Calculate the predicted labels of the data points in the training set # using training data to find predicted y\n",
    "        pred_train = reg.predict(X_train[:,:(i+1)]) \n",
    "        err_train[i] = mean_squared_error(y_train, pred_train)\n",
    "        pred_val = reg.predict(X_val[:,:(i+1)]) \n",
    "        err_val[i] = mean_squared_error(y_val, pred_val)\n",
    "        print(\"X:\\n\",X_train[:,:(i+1)])\n",
    "        print('pred_train:\\n ',pred_train)\n",
    "        print('err_train:\\n', err_train)\n",
    "        print('pred_val:\\n' , pred_val)\n",
    "        print('err_val:\\n' ,  err_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e4160b46232375488f4d817a7fae7327",
     "grade": false,
     "grade_id": "cell-13d40bbc019a9337",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='kfold'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Student task 3.3. 5-Fold Cross Validation.\n",
    "     \n",
    "The purpose of the code snippet below is to compute the training and validation errors for each choice of $r=1,\\ldots,n$ using 5-fold cross-validation. Your task is to complete the part of the loop that performs 5-fold cross-validation using the `KFold` class in scikit-learn. For each $r$ you should\n",
    "    \n",
    "1. Iterate over the `K` pairs of train and validation indices and for each pair, calculate the training and validation errors of a linear regression model (with `fit_intercept=True`) and store them in  `err_train_splits` and `err_val_splits` respectively.\n",
    "    \n",
    "    \n",
    "2. Calculate the average training- and validation errors and store these at index `r` in the arrays `err_train` and `err_val` (both of shape $(n, )$) respectively.\n",
    "\n",
    "Note: do NOT use `get_train_val_errors()` function you defined in previous student task.    \n",
    "    \n",
    "For more information, see the scikit-learn [documentation of KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html).\n",
    "\n",
    "Afterwards, the training- and validation errors are plotted for comparison with the errors from the previous student task.\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c8851e06bf7284e1e865e8cdca1e08e",
     "grade": false,
     "grade_id": "cell-4012bc9df7f9d476",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y training data: (16,)\n",
      "y validation data: (4,)\n",
      "(16, 10)\n",
      "(4, 10)\n",
      "[24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2]\n",
      "Training errors for each K:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n",
      "Validation error for each K:\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 10 # max number of features\n",
    "X, y = load_housing_data(n=n)  # read in 20 data points with n features \n",
    "\n",
    "err_train = np.zeros(n)  # Array to store training errors\n",
    "err_val = np.zeros(n)  # Array to store validation errors\n",
    "\n",
    "K = 5  # Number of splits\n",
    "kf = KFold(n_splits=K, shuffle=False)    # Create a KFold object with 'K' splits\n",
    "\n",
    "for r in range(n):\n",
    "    err_train_splits = []  # List for storing the training errors for the splits\n",
    "    err_val_splits = []  # List for storing the validation errors for the splits\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    reg = LinearRegression(fit_intercept=True)\n",
    "    iteration = 0\n",
    "    for train_indices, val_indices in kf.split(X):\n",
    "    \n",
    "        iteration += 1\n",
    "        X_train = X[train_indices,:]    # Get the training set    \n",
    "        X_val = X[val_indices,:]    # Get the validation set\n",
    "        y_train=y[train_indices]\n",
    "        y_val=y[val_indices]\n",
    "\n",
    "\n",
    "print(\"y training data:\",y_train.shape)\n",
    "print(\"y validation data:\",y_val.shape)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y)\n",
    "        \n",
    "print('Training errors for each K:')\n",
    "print(err_train, '\\n')\n",
    "print('Validation error for each K:')\n",
    "print(err_val, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1d59b1de0357ed5a3632f2b2bfe3e98",
     "grade": true,
     "grade_id": "cell-d2c2dd1d2717b4ce",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The second element of err_val should be smaller than the first element!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m err_train\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (n,), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merr_train is of the wrong shape!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m err_val\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (n,), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merr_val is of the wrong shape!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m err_val[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m err_val[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe second element of err_val should be smaller than the first element!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSanity checks passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: The second element of err_val should be smaller than the first element!"
     ]
    }
   ],
   "source": [
    "# Perform sanity checks on the outputs\n",
    "assert err_train.shape == (n,), \"err_train is of the wrong shape!\"\n",
    "assert err_val.shape == (n,), \"err_val is of the wrong shape!\"\n",
    "assert err_val[0] > err_val[1], \"The second element of err_val should be smaller than the first element!\"\n",
    "\n",
    "print(\"Sanity checks passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d49900a614db0d867a812b0efd3e35f8",
     "grade": true,
     "grade_id": "cell-e96f739108885230",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de5f50f4dc0631d9a90c6f45141b68c1",
     "grade": false,
     "grade_id": "cell-808ea91b16756afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training and validation errors for the different number of features r\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(range(1, n+1), err_train, color='black', label=r'$E_{\\rm train}(r)$', marker='o')  # Plot training error\n",
    "plt.plot(range(1, n+1), err_val, color='red', label=r'$E_{\\rm val}(r)$', marker='x')  # Plot validation error\n",
    "\n",
    "plt.title('5-fold training and validation errors for different number of features', fontsize=16)    # Set title\n",
    "plt.ylabel('Empirical error')    # Set label for y-axis\n",
    "plt.xlabel('r features')         # Set label for x-axis\n",
    "plt.xticks(range(1, n+1))      # Set the tick labels on the x-axis to be 1,...,n\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3de85d28d603d236a0bca4c91f7c732",
     "grade": false,
     "grade_id": "cell-44a5f961cd26285f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you have completed the task correctly, you should see plot similar to this one:\n",
    "\n",
    "<img src=\"../../../coursedata/3_ModelValSel/train_val_kfold.png\" width=500/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7e646427fa19af8321939481f811c904",
     "grade": false,
     "grade_id": "cell-12f1478400587745",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If we compare the figure above to the one in the previous student task, we can see that the validation error obtained by 5-fold cross-validation seems to provide a more realistic estimate of the model's performance on new data. The validation error is now consistently larger than the training error, and the validation error seems less erratic. In practice, it is almost always preferable to use K-fold cross-validation instead of a single validation split for model validation and selection due to the increased robustness of the validation error.\n",
    "\n",
    "With regards to the apartment price problem, we can conclude with reasonable confidence that the model with the best performance on new data points is the one using only the number of rooms and the percentage of apartments in the neighbourhood constructed before 1970 as its features. As such, it would seem that the experimental features in the dataset were not beneficial after all.\n",
    "\n",
    "This example problem highlights the importance of using proper model validation. Since ML models nearly always overfit to some degree, it is important to estimate the true performance of the model before practical use by using model validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "186bd2dcdbe6b0538a21fcd9cbaae5cc",
     "grade": false,
     "grade_id": "cell-b77c4a1ec9d385a4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Estimating the accuracy of the selected model on a separate test set\n",
    "\n",
    "We can use validation error to select the model with the best performance on new data out of many candidate models. In most applications, we are also interested in obtaining an estimate of the performance of the selected model on new data. To this end, it would seem convenient to use the validation error from the model selection phase, but unfortunately this generally results in an overly optimistic estimate.\n",
    "\n",
    "The validation error is, in general, an excessively optimistic estimate of the performance of the final model **since the model is chosen based on this value**. We can persuade ourselves of the validity of this statement by imagining two different models with a negligible difference in the expected error on new data points. **If we select one of these models based on the models' validation errors on a finite validation set, we will end up selecting the model that happens to fit the validation set better**.\n",
    "\n",
    "The solution to this is to use a separate dataset, frequently referred to as the **test set**, for testing the performance of the final model. In practice, the test set is most often obtained by selecting a part of the original dataset for this purpose. The rest of the data points are then used to train and validate the different model candidates using the methods described in this notebook.\n",
    "\n",
    "As a final note, it is good to emphasize that the separate test set is only necessary when we have selected a model based on the validation error. If we are only interested in obtaining the performance estimate for one model, a single validation/test set suffices.\n",
    "\n",
    "## Key takeaways\n",
    "\n",
    "- In model selection (or hyperparameter tuning), we would ideally like to choose the model with the smallest ***expected*** error. However, since this is not possible (we don't know the true data distribution) we use validation error as a tractable alternative - it is our hope that the validation error reflects the true expected error more faithfully (compared to training error). Test error is similarly motivated, with the central difference that we are not using it to select a model, but rather to give us an estimate for the true performance of an already trained (fixed) model.\n",
    "\n",
    "\n",
    "- Any data that has had an influence on the model (through training, tuning or selection) can no longer be used to provide you with a reliable estimate of the model's (true) performance (on the unseen data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2c6ab491800681e9b8b4c27390ea25cf",
     "grade": false,
     "grade_id": "cell-a451b1c380ed82da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "##  Regularization\n",
    "\n",
    "In the previous sections, we considered how to validate trained ML models to get high-quality estimates of their predictive capabilities. In the remainder of the notebook, we consider a tangential question - how to train an ML model so that the learned predictor generalizes better to new data than the predictor minimizing the average loss. \n",
    "\n",
    "Consider an ML method based on a large hypothesis space such as polynomials with a large degree. Large hypothesis spaces typically contain complex predictors that achieve very low training errors by overfitting the data. Thus, if we search for the optimal predictor in this hypothesis space (i.e., train the model) by minimizing the training error, we will obtain a predictor that overfits the training data and generalizes poorly to other data. However, there might be predictors in the hypothesis space that generalize much better to new data even though they have a higher average loss than the learned predictor. In order to learn these predictors, the model must be trained by minimizing some other quantity than the average loss.\n",
    "\n",
    "**Regularization** is a technique in which models are trained by minimizing a **cost function** that penalizes the complexity of a predictor function $h$. The cost function that is minimized when training a regularized model is composed of the average loss and an additional **penalty term**:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{E}(h) = \\underbrace{\\underbrace{(1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2}}_{\\mbox{ average loss}} + \\underbrace{\\alpha \\mathcal{R}(h)}_{\\mbox{anticipated increase of error (loss) on new data}}}_{\\mbox{ estimate (approximation) of validation error }}.  \n",
    "\\end{equation}\n",
    "\n",
    "The central idea of regularization is that the penalized cost function is minimized by a less complex predictor than the average loss. Thus, a model trained using the penalized cost function should, in general, have better generalization capabilities provided that the penalty term is well chosen. \n",
    "\n",
    "The penalty term itself is composed of two factors - a **regularization term** $\\mathcal{R}(h)$ and a scaling factor $\\alpha$. The former quantifies a function's complexity, and the latter scales the penalty by a specified factor. Effectively, $\\alpha$ **offers a trade-off between the prediction error (training error) incurred on the training data and the complexity of a predictor**. Large $\\alpha$ favor less complex predictor functions, while small $\\alpha$ put more emphasis on obtaining a small average loss.\n",
    "\n",
    "\n",
    "### The regularization term (choosing $\\mathcal{R}(h)$)\n",
    "\n",
    "In order to implement regularization in practice, we need to choose a regularization term $\\mathcal{R}$ that quantifies the complexity of predictor functions in an appropriate way. Two widely used choices are the $\\ell_1$ norm \n",
    "\n",
    "\\begin{equation}\n",
    "\\|\\mathbf{w} \\|_1 = \\sum_{i=1}^n |w_i|= |w_1| + |w_2| + \\ldots + |w_n|\n",
    "\\end{equation}\n",
    "\n",
    "and the squared $\\ell_2$ norm\n",
    "\n",
    "\\begin{equation}\n",
    "\\|\\mathbf{w} \\|_2^2 = \\sum_{i=1}^n w_i^2 = w_1^2 + w_2^2 + \\ldots + w_n^2.\n",
    "\\end{equation}\n",
    "\n",
    "Both of these alternatives are based on the premise that the complexity of a predictor increases with the magnitude of its parameters $w$.\n",
    "\n",
    "### Hyperparameter tuning (choosing $\\alpha$)\n",
    "\n",
    "The $\\alpha$ factor in the regularized cost function is a **hyperparameter** of the regularized model. In contrast to **model parameters**, hyperparameters are not optimized by training the model but must be defined in advance. The hyperparameters' values are typically chosen by selecting the value from a set of candidates that results in the lowest validation error for the trained model. This process is called **hyperparameter tuning** and can be seen as a case of model selection, in which the models differ by the values of the hyperparameters.\n",
    "\n",
    "The hyperparameter tuning process for $\\alpha$ proceeds roughly as follows:\n",
    "1. we specify a list of candidate values for $\\alpha$, \n",
    "2. for each choice of $\\alpha$, we learn a predictor that minimizes the regularized cost function\n",
    "3. for each choice of $\\alpha$, we validate the trained predictor $h^{(\\alpha)}_{\\rm opt}$ by computing the validation error\n",
    "\n",
    "\\begin{equation}\n",
    "E_{\\rm val}^{(\\alpha)} = (1/m_{\\rm v}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(v)}} \\big(y^{(i)} - h^{(\\alpha)}_{\\rm opt}(\\mathbf{x}^{(i)})\\big)^{2}.\n",
    "\\end{equation}\n",
    "\n",
    "4. We select the value of $\\alpha$ with smallest validation error to be used in our final model \n",
    "\n",
    "Next, we will briefly consider two linear models that use different regularization terms $\\mathcal{R}(h)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a0f3d80fe10b60babb38322a2f99dbce",
     "grade": false,
     "grade_id": "cell-752e47f60e6dcf8f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Ridge Regression\n",
    "\n",
    "Ridge regression is a linear model that uses the same linear hypothesis space as the standard linear regression model, but learns the optimal predictor by minimizing the penalized cost function with $\\mathcal{R}(h)=\\|\\mathbf{w}\\|_2^2=w_1^2 + w_2^2 + \\ldots + w_n^2$. The cost function is \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{E}(\\mathbf{w}, w_0) = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - w_0 - \\mathbf{w}^T\\mathbf{x}^{(i)} \\big)^{2} + \\alpha\\|\\mathbf{w}\\|_2^2.  \n",
    "\\end{equation}\n",
    "\n",
    "Since the regression weights are squared in the penalty term, Ridge regression harshly penalizes predictors with large individual feature weights. This feature makes ridge regression particularly useful for ML problems with many highly correlated features.\n",
    "\n",
    "The presence of such features is called **multicollinearity**, and is associated with an unstable optimal predictor that is highly sensitive to the training data and thus generalizes poorly to new data. The instability stems from the fact that when the features are highly correlated, a large positive weight on feature can be \"canceled out\" by large negative weight on another. Ridge regression mitigates this problem by penalizing predictors with large individual feature weights and favoring predictors that have a better generalization capability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0986bb6186e1eac01f99cf9d7646fb70",
     "grade": false,
     "grade_id": "cell-a697f48e6cf7e933",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='ridgeReg'></a>\n",
    "<div class=\" alert alert-info\">\n",
    "\n",
    "### Demo. Ridge Regression. \n",
    "\n",
    "    \n",
    "A ridge regression model can be fitted to a dataset with scikit-learn by using the function `Ridge.fit()`. After fitting the model, the optimal weight vector $\\mathbf{w}_{\\rm opt}$ is stored in the attribute `Ridge.coef_` of the `Ridge` instance. \n",
    "\n",
    "[See documentation of Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)\n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83c95ac93da8b9da76ad797390a67977",
     "grade": false,
     "grade_id": "cell-ef05953e6a07a985",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: \n",
      " [ 1.62648008 -0.13079883  0.8888383  -0.58619617 -0.98729306  0.28057208\n",
      " -0.09922109  0.93619655 -0.14894959  0.13927532]\n",
      "Training error: \n",
      " 13.682054265430263\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "n = 10\n",
    "X, y = load_housing_data(n)\n",
    "# 80% training and 20% val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  \n",
    "\n",
    "alpha = 10    # Define value of the regularization parameter 'alpha'\n",
    "\n",
    "ridge = Ridge(alpha=alpha, fit_intercept=True)    # Create Ridge regression model\n",
    "ridge.fit(X_train, y_train)                       # Fit the Ridge regression model on the training set\n",
    "y_pred = ridge.predict(X_train)                   # Predict the labels of the training set\n",
    "w_opt = ridge.coef_                               # Get the optimal weights (regression coefficients) of the fitted model\n",
    "err_train = mean_squared_error(y_pred, y_train)   # Calculate the training error\n",
    "\n",
    "# Print optimal weights and training error\n",
    "print('Optimal weights: \\n', w_opt)\n",
    "print('Training error: \\n', err_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7d6b6b49344200b490ec49b68f8dddd0",
     "grade": false,
     "grade_id": "cell-63208d67670e9841",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Lasso\n",
    "\n",
    "Lasso is a linear model that uses the same hypothesis space as the standard linear regression model, but learns the optimal predictor by minimizing the regularized cost function with $\\mathcal{R}(h)=\\|\\mathbf{w}\\|_1=|w_1| + |w_2| + \\ldots + |w_n|$. The cost function is of the form \n",
    "\n",
    "\\begin{equation}\n",
    " \\mathcal{E}(\\mathbf{w}, w_0) = (1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - w_0 - \\mathbf{w}^T\\mathbf{x}^{(i)} \\big)^{2} + \\alpha\\|\\mathbf{w}\\|_1.  \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "In contrast to Ridge regression, Lasso does not square the feature weights in the penalty, and thus, both large and small feature weights are penalized proportionately. In practice, this often results in the model learning a predictor with some zero-valued regression weights since smaller weights are not penalized less than larger ones. As such, Lasso can be interpreted as a form of automatic feature selection!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9af70ab847986e0388abaf3f4da51cd5",
     "grade": false,
     "grade_id": "cell-b63b8bd46646688d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='lassoReg'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Student Task 3.4. Lasso Regression.\n",
    "\n",
    "Complete the function `fit_lasso` that uses the Scikit-learn function `Lasso.fit()` to compute the optimal predictor for $\\alpha=$ `alpha_val`. When initializing Lasso, please use `fit_intercept=True`. This function is then used find the optimal Lasso predictor for $\\alpha = 1$.\n",
    "\n",
    "[Documentation for Lasso in Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c5ee3bf06eeeb0532c41cfdb2eeed045",
     "grade": false,
     "grade_id": "cell-b6cd24ae92c6b545",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "X, y = load_housing_data(n)    \n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)    \n",
    "\n",
    "def fit_lasso(X, y, alpha_val):\n",
    "    ### STUDENT TASK ###\n",
    "    # .\n",
    "    # .\n",
    "    # .\n",
    "    # w_opt = ...\n",
    "    # error = ...\n",
    "    # YOUR CODE HERE\n",
    "    alpha_val = 1    # Define value of the regularization parameter 'alpha'\n",
    "\n",
    "\n",
    "    lasso = Lasso(alpha=alpha_val, fit_intercept=True)    # Create Ridge regression model\n",
    "    lasso.fit(X_train, y_train)                       # Fit the Ridge regression model on the training set\n",
    "    y_pred = lasso.predict(X_train)                   # Predict the labels of the training set\n",
    "    w_opt = lasso.coef_                               # Get the optimal weights (regression coefficients) of the fitted model\n",
    "    error = mean_squared_error(y_pred, y_train)   # Calculate the training error\n",
    "    \n",
    "\n",
    "    # return optimal coefficient and MSE \n",
    "    return w_opt, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: \n",
      " [ 3.56106346 -0.13171416  0.         -0.         -0.          0.\n",
      " -0.          0.02624124  0.          0.        ]\n",
      "Training error: \n",
      " 14.731838524845271\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "n = 10\n",
    "X, y = load_housing_data(n)\n",
    "# 80% training and 20% val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=2)  \n",
    "\n",
    "alpha = 1    # Define value of the regularization parameter 'alpha'\n",
    "\n",
    "lasso = Lasso(alpha=alpha, fit_intercept=True)    # Create Ridge regression model\n",
    "lasso.fit(X_train, y_train)                       # Fit the Ridge regression model on the training set\n",
    "y_pred = lasso.predict(X_train)                   # Predict the labels of the training set\n",
    "w_opt = lasso.coef_                               # Get the optimal weights (regression coefficients) of the fitted model\n",
    "err_train = mean_squared_error(y_pred, y_train)   # Calculate the training error\n",
    "\n",
    "# Print optimal weights and training error\n",
    "print('Optimal weights: \\n', w_opt)\n",
    "print('Training error: \\n', err_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dc55b561caef0427cc994a6eddedd6e",
     "grade": false,
     "grade_id": "cell-8545851497bec884",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: \n",
      " [ 3.56106346 -0.13171416  0.         -0.         -0.          0.\n",
      " -0.          0.02624124  0.          0.        ]\n",
      "Training error: \n",
      " 14.731838524845271\n",
      "\n",
      "Sanity check tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Set alpha value\n",
    "alpha_val = 1\n",
    "\n",
    "# Fit Lasso and calculate optimal weights and training error \n",
    "w_opt, training_error = fit_lasso(X_train, y_train, alpha_val)\n",
    "\n",
    "# Print optimal weights and the corresponding training error\n",
    "print('Optimal weights: \\n', w_opt)\n",
    "print('Training error: \\n', training_error)\n",
    "\n",
    "# Perform some sanity checks on the outputs\n",
    "assert w_opt.reshape(-1,1).shape == (10,1), \"'w_opt' has wrong shape\"\n",
    "assert int(w_opt.sum()) == 3, \"'w_opt' has wrong values\"\n",
    "assert np.isscalar(training_error), \"'training_error' is not scalar\"\n",
    "assert training_error < 1000, \"'training_error' is too large\"\n",
    "print('\\nSanity check tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f93879b000314d60b3ae0ee6bd97918a",
     "grade": true,
     "grade_id": "cell-44c7f2c09ba11e52",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f933981421284b5d23ce3d1a37115b83",
     "grade": true,
     "grade_id": "cell-1a0bccc4f1816064",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e9e4470177cd9ba9d6d1d2e2d6befb0",
     "grade": false,
     "grade_id": "cell-1355f27ba4fd6ca5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "When using lasso or ridge regression, we need to find a suitable value for the regularization parameter $\\alpha$. We can do this by using the hyperparameter tuning scheme presented earlier. This type of hyperparameter tuning is often referred to as **grid search**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a59a75f40af877242036c65b48ec408f",
     "grade": false,
     "grade_id": "cell-8e4d019532f4bff3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='lassoParameter'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "### Student Task 3.5. Tuning the Regularization Parameter in the Lasso.\n",
    "    \n",
    "In this task, your objective is to calculate the training and validation errors of a Lasso model with different values of $\\alpha$ using grid-search. The training and validation errors are to be calculated using 5-fold cross-validation. To this end, you will use an extremely useful class in Scikit-learn: [GridSeachCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) \n",
    "    \n",
    "`GridSearchCV` takes as input an estimator object (i.e., a Scikit-learn model such as `LinearRegression`) and a dictionary containing key-value pairs, where the key is the name of some parameter of the estimator and the value is a list of candidate values for that parameter. It also takes other optional parameters, such as the number of data splits `cv` in cross-validation.\n",
    "    \n",
    "The grid-search itself is performed by calling the function `GridSearchCV.fit(X, y)`. After calculating validation scores (~errors) for the different parameter values, the function fits the model with the best parameter value. The results of the parameter search, including validation errors, are saved in the attribute `.cv_results_`.\n",
    "    \n",
    "To solve this task using `GridSearchCV`, you should\n",
    "    \n",
    "1. Create a [dictionary](https://realpython.com/python-dicts/) containing one key-value pair with the candidate values `alpha_values` for the parameter `alpha`\n",
    "    \n",
    "    \n",
    "2. Create a `Lasso` object without defining the parameter `alpha`\n",
    "    \n",
    "    \n",
    "3. Create a `GridSearchCV` object with the `Lasso` object and the dictionary as inputs. Furthermore, you must define the parameters `scoring='neg_mean_squared_error'`, `cv=5`, and `return_train_score=True`. Here, `scoring` defines the metric with which training and validation errors are calculated, `cv` the number of splits in the $K$-fold cross-validation, and `return_train_score` whether or not training errors are calculated.\n",
    "    \n",
    "    \n",
    "4. Use the function `GridSearchCV.fit(X,y)` to calculate training and validation errors for different parameter values and subsequently fit the model with the best $\\alpha$. The function takes as input the entire feature matrix `X` and label vector `y`. \n",
    "    \n",
    "    \n",
    "5. Save the training and validation errors in numpy arrays `err_train` and `err_val`, respectively. The grid-search results are found in the attribute `cv_results_` of the `GridSearchCV` object. The results are stored in a dict, and the training and validation scores are accessed with the keys `mean_train_score` and `mean_test_score`. `GridSearchCV` uses negative MSE instead of MSE for implementational reasons, so you must remember to turn the errors positive when defining `err_train` and `err_val`.\n",
    "    \n",
    "\n",
    "**Hint:** Find example of how `GridSearchCV` is used [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), section \"Examples\".\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e297c71b7af5d3abb19e8753e5d8a52f",
     "grade": false,
     "grade_id": "cell-f087a0a92f393d73",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "alpha_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "### STUDENT TASK ###\n",
    "# 1. Create a dictionary \n",
    "# params = ...\n",
    "\n",
    "# 2. Create a Lasso object\n",
    "# lasso = ...\n",
    "\n",
    "# 3. Create a GridSearchCV object\n",
    "# cv = ...\n",
    "\n",
    "# 4. fit a GridSearchCV object to data (X,y)\n",
    "# cv.fit(...)\n",
    "\n",
    "# 5. retrieve training and validation errors from fitted GridSearchCV object (stored in attribute `.cv_results_`)\n",
    "# err_train = ...\n",
    "# err_val = ...\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51def4f473d188030bce6855c49c1494",
     "grade": false,
     "grade_id": "cell-b9c98c1fb7c77c74",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Perform some sanity checks on the outputs\n",
    "assert err_train[0] > 0 and err_val[0] > 0, \"Errors are negative!\"\n",
    "assert err_train.shape == (len(alpha_values),), \"'err_train' has wrong shape\"\n",
    "assert err_val.shape == (len(alpha_values),), \"'err_val' has wrong shape\"\n",
    "\n",
    "assert int(err_train.sum()) == 86,  \"'err_train' has wrong values\"\n",
    "assert int(err_val.sum()) == 200,  \"'err_val' has wrong values\"\n",
    "\n",
    "print('Sanity check tests passed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bc64ee8995ecd0b16642e4db3c0a5a2",
     "grade": true,
     "grade_id": "cell-e05df1c0800623d3",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "898fc81948f3c6433391f7a507f115f7",
     "grade": true,
     "grade_id": "cell-0754a0391a845e43",
     "locked": true,
     "points": 1.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# hidden tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ecd95f1571b84d4f69f472e57d1c68d",
     "grade": false,
     "grade_id": "cell-7049b872139fc5ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Plot the training and validation errors\n",
    "plt.figure(figsize=(8,4))    # Set figure size\n",
    "plt.plot(alpha_values, err_train, marker='o', color='black', label='training error')    # Plot training errors\n",
    "plt.plot(alpha_values, err_val, marker='o', color='red', label='validation error')    # Plot validation errors\n",
    "plt.xscale('log')    # Set x-axis to logarithmic scale\n",
    "plt.xlabel(r'$\\alpha$')    # Set label of x-axis\n",
    "plt.ylabel(r'$E(\\alpha)$')    # Set label of y-axis\n",
    "plt.title(r'Errors with respect to $\\alpha$', fontsize=16)    # Set title\n",
    "plt.legend()    # Show legend\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "77ce7bb614e2cec3c5aedc7e8ac2aa23",
     "grade": false,
     "grade_id": "cell-046088f45a31c619",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the figure above, we can see that the optimal value for $\\alpha$ is $0.1$ since this value results in the lowest validation error for the Lasso model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1c5bc952d9b93a6f6716bab32d56d3e",
     "grade": false,
     "grade_id": "cell-82e39e7e12da5e63",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Quiz Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b4276e267a6ca4b9401a580e0bbb147",
     "grade": false,
     "grade_id": "cell-1fb3f77c938bdb0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='Question3_1'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question 1 </p>\n",
    "\n",
    "<p>What is the goal of model selection in machine learning?</p>\n",
    "\n",
    "<ol>\n",
    "  <li> To choose (learn) the optimal predictor function $h_{\\rm opt}$ out of a given hypothesis space (model) $\\mathcal{H}$.</li>\n",
    "  <li> To select the most suitable car model using machine learning methods.</li>\n",
    "  <li> To select the optimal weights used for regularization.</li>\n",
    "  <li> To select the best hypothesis space out of a set of candidates $\\lbrace \\mathcal{H}^{(1)}, \\mathcal{H}^{(2)}, \\ldots,\\mathcal{H}^{(n)} \\rbrace$.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "571fbc33ef6e7910f9dd187a9e4fc0d4",
     "grade": false,
     "grade_id": "cell-c79e73bca1155558",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "answer_Q1  = 4\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c81bf3b1262396fce0351df6ee6ee06",
     "grade": true,
     "grade_id": "cell-b35cc9f75666b8ee",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check tests passed!\n"
     ]
    }
   ],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_Q1 in [1,2,3,4], '\"answer_Q1\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfac4d41e9904a53116b900a3f95be94",
     "grade": false,
     "grade_id": "cell-5e154c576a4ff600",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR3_2'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question 2. </p>\n",
    "\n",
    "<p>What is a good measure for the prediction error (loss) incurred by a predictor function $h(\\mathbf{x})$ on new data points?</p>\n",
    "<ol>\n",
    "  <li> The empirical error (average loss) of $h(\\mathbf{x})$ on the <b>training set</b> which is also used to tune $h(\\mathbf{x})$. </li>\n",
    "  <li> The empirical error (average loss) of $h(\\mathbf{x})$ on some <b>validation set</b> which is different from the training set. \n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b850c3a48d4cc3d2b2170d611a67fe12",
     "grade": false,
     "grade_id": "cell-897738e907d8a93b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "answer_Q2  = 2\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53c495b132a18b8ee54ea0e814d00027",
     "grade": true,
     "grade_id": "cell-95bf07d58bed5744",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check tests passed!\n"
     ]
    }
   ],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_Q2 in [1,2], '\"answer_Q2\" Value should be an integer between 1 and 2.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bab63b720733fbf6f748e8bb6cf2bbf3",
     "grade": false,
     "grade_id": "cell-f55ccbeceab15cbf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='QuestionR3_3'></a>\n",
    "<div class=\" alert alert-warning\">\n",
    "<p><b>Student Task.</b> Question 3. </p>\n",
    "\n",
    "Regularized linear regression amounts to finding the predictor $h(\\mathbf{x})$ which minimizes the regularized training error \n",
    "\\begin{equation} \n",
    "(1/m_{t}) \\sum_{\\big(\\mathbf{x}^{(i)},y^{(i)}\\big) \\in \\mathbb{X}^{(t)}} \\big(y^{(i)} - h(\\mathbf{x}^{(i)}) \\big)^{2} + \\alpha \\mathcal{R}(h).\n",
    "\\end{equation}\n",
    "Which statement is true?\n",
    "\n",
    "<ol>\n",
    "  <li> Using a large value for the regularization parameter $\\alpha$ prefers predictors with large complexity $\\mathcal{R}(h)$ but small training error.</li>\n",
    "  <li>  Using a small value for the regularization parameter $\\alpha$ prefers predictors with large complexity $\\mathcal{R}(h)$ but small training error.</li>\n",
    "  <li> For regularization parameter $\\alpha=0$, the optimal predictor is always $h(\\mathbf{x}) =0$. </li>\n",
    "  <li> For regularization parameter $\\alpha=0$, the optimal predictor is always $h(\\mathbf{x}) =42$.</li>\n",
    "</ol> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a8b350ca69f0828f320eeb45648457c9",
     "grade": false,
     "grade_id": "cell-c14120807b0b28a6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### STUDENT TASK ###\n",
    "# remove the line raise NotImplementedError() before testing your solution and submitting code\n",
    "answer_Q3  = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "068c82dc4e0fd1de4da7268cb986593a",
     "grade": true,
     "grade_id": "cell-05d01877f74799bd",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check tests passed!\n"
     ]
    }
   ],
   "source": [
    "# This cell is for tests\n",
    "\n",
    "assert answer_Q3 in [1,2,3,4], '\"answer_Q3\" Value should be an integer between 1 and 4.'\n",
    "print('Sanity check tests passed!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "3"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 4,
           "op": "addrange",
           "valuelist": "9"
          },
          {
           "key": 4,
           "length": 1,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "version",
       "op": "patch"
      }
     ],
     "key": "language_info",
     "op": "patch"
    }
   ]
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
