{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b50a60e5-4be1-4376-847a-35472e0d1367",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Developed to learn to model various temporal processes such as\n",
    "\n",
    " * Stock market prices\n",
    " * Weather forecast\n",
    " * etc.\n",
    "\n",
    "but finally led to major breakthroughs in\n",
    "\n",
    " * [Natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing)\n",
    "\n",
    "once researchers started to use RNNs in neural machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e37d1d-bf63-448a-9068-65acba941752",
   "metadata": {},
   "source": [
    "## Generative learning\n",
    "\n",
    "The principal idea is to use a sequence of known inputs to generate an output. By including the generated output to the known inputs, a sequence of infinite length can be generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67077ac4-a8ca-47a3-b5e4-77019d832c24",
   "metadata": {},
   "source": [
    "### Jordan RNN (1986)\n",
    "\n",
    "There was substantial work on the topic at the end of 1980's. For example, control theory and dynamic systems provided a suitable existing framework to study recurrent neural networks. See, for example,\n",
    "\n",
    " * A.J. Robinson and F. Fallside (1987): \"Static and Dynamic Error Propagation Networks with Application to Speech Coding\". In the Proceedings of the Neural Information Processing Systems (NeurIPS). [PDF](https://proceedings.neurips.cc/paper/1987/file/a1d0c6e83f027327d8461063f4ac58a6-Paper.pdf)\n",
    " \n",
    "The main finding was so called [\"Backpropagation through time\"](https://en.wikipedia.org/wiki/Backpropagation_through_time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2318df-f1d3-455b-ac90-cab880355e34",
   "metadata": {},
   "source": [
    "### Elman RNN (1990)\n",
    "\n",
    "The first with some good practical results. Developed during the previous wave on neural network research:\n",
    "\n",
    " * J.L. Elman (1990): Finding structure in time, In Cognitive Sience, Vol. 14, [DOI Link](https://doi.org/10.1016/0364-0213(90)90002-E)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc500e8d-8423-4229-a11d-0f2fa476d1d7",
   "metadata": {},
   "source": [
    "**Example:** Learn to generate sinusoida wave with Elman RNN\n",
    "\n",
    "See the Codelab notebook:\n",
    "\n",
    " * https://colab.research.google.com/drive/1uZLe9BN9Uu6kT6sZvS_SP6syzk0B_wja?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8834b36-c5f5-49e5-b580-cce8c1f69d5e",
   "metadata": {},
   "source": [
    "### Long Short-term Memory (LSTM)\n",
    "\n",
    "The original idea was proposed in\n",
    "\n",
    " * S. Hochreiter, J. Schmidhuber (1997): \"Long Short-term Memory\", Neural Computation, Vol 9 No 8. [PDF](https://ieeexplore.ieee.org/abstract/document/6795963)\n",
    "\n",
    "More details in this excellent Blog post\n",
    "\n",
    " * https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "The main motivation of the LSTMs vs. the previous RNNs such as Elman is that they are better able to solve the vanishing (or exploding) gradient problem:\n",
    "\n",
    " * https://medium.datadriveninvestor.com/how-do-lstm-networks-solve-the-problem-of-vanishing-gradients-a6784971a577?gi=8cc7cd9d3e1f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9d6de4-c0ba-4954-a8d5-428620fd477a",
   "metadata": {},
   "source": [
    "**Example:** Learn to generate sinusoida wave with LSTM RNN\n",
    "\n",
    "See the Codelab notebook:\n",
    "\n",
    " * https://colab.research.google.com/drive/1uZLe9BN9Uu6kT6sZvS_SP6syzk0B_wja?usp=sharing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6e5fd1-2d9b-48fe-8cab-4b7805ec48a2",
   "metadata": {},
   "source": [
    "**Example:** Generate text with LSTM RNN\n",
    "\n",
    "See the Codelab notebook:\n",
    "\n",
    " * https://colab.research.google.com/drive/1jbFTXyBFe64J2xtpek-OjsCytqn0OY8Y?usp=sharing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c89d43b-f46e-4b33-a50a-28a01cf53db1",
   "metadata": {},
   "source": [
    "## Sequence-to-sequence learning\n",
    "\n",
    "Seq2Seq models differ from generative learning in the sense that inputs and outputs can be of different type and length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a3338c-f94f-4c97-83f7-5a72fa564cd9",
   "metadata": {},
   "source": [
    "### Neural machine translation\n",
    "\n",
    "The two sequential tasks related to natural language processing (NLP) are word prediction and neural translation. For the neural translation a different structure was proposed than to word prediction. That structure led to the idea of attention and that to the idea of the Transformer structure. Transformer solves the problem of machine translation through word prediction learning (generative AI) which makes it interesting as this capability \"emerges\" from the next word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cb7d1d-6777-46bd-99ab-f019963c1762",
   "metadata": {},
   "source": [
    "### Encoder-decoder RNN\n",
    "\n",
    "Idea was originally published in\n",
    "\n",
    " * I. Sutskever, O. Vinyals, Q.V. Le (2014): \"Sequence to Sequence Learning with Neural Networks\", NeurIPS 2014. [PDF](https://doi.org/10.48550/arXiv.1409.3215)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7eba4-00c8-4f17-9f19-a1b0f060d361",
   "metadata": {},
   "source": [
    "**Example:** Machine translation with encoder-decoder LSTML\n",
    "\n",
    "See the Colab notebook:\n",
    "\n",
    " * [Colab](https://colab.research.google.com/drive/1dP6pBcxwZMd1ZeE1poOPg63yhwKfwjxL?usp=sharing)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44480fee-22f8-43ef-bf87-91dfe87a9271",
   "metadata": {},
   "source": [
    "### Attention\n",
    "\n",
    "Attention mechanism for neural translation was proposed in\n",
    "\n",
    " * D. Bahdanau, K. Cho, Y. Bengio (2015): \"Neural Machine Translation by Jointly Learning to Align and Translate\" in Proc. of the Int'l Conf. on Learning Representations (ICLR) [PDF](https://arxiv.org/abs/1409.0473)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b92ec62-a06d-4e47-a5f6-269ba6d23859",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "\n",
    "Finally the Transformer architecture was introduced in\n",
    "\n",
    " * A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, Ł. Kaiser, I. (2017): \"Attention Is All You Need\". In Proceedings of the Neural Information Processing Systems (NeurIPS). [PDF](https://papers.nips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c2b4c5-3a3a-4bf9-9c4c-30adbefdeb7f",
   "metadata": {},
   "source": [
    "## Foundation models\n",
    "\n",
    "Some year after the Transformer was developed, it was used to show that generative pre-training has many downstream tasks that can be solved by fine-tuning (cf. backbone networs) - a seminal work was the GPT-1 paper\n",
    "\n",
    " * A. Radford, K. Narasimhan, T. Salimans, I. Sutskever (2018): \"Improving language understanding by generative pre-training\" OpenAI technical report. [PDF](https://www.mikecaptain.com/resources/pdf/GPT-1.pdf)\n",
    "\n",
    "This led to development of multiple other \"foundation models\" that are trained self-supervised and from where other capabilities emerge through vast amount of data and huge models.\n",
    "\n",
    " * Images (Dall-E by OpenAI)\n",
    " * Music (MusicLM by Google)\n",
    " * Videos (Sora by OpenAI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40d9ab-96f0-456d-bf85-bd2d11d77aba",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " * [Wikipedia page](https://en.wikipedia.org/wiki/Recurrent_neural_network) provides excellent historical review.\n",
    "\n",
    " * I. Goodfellow, Y. Bengio and A. Courville (2016): Deep Learning, MIT Press [Web](https://www.deeplearningbook.org/) provides good modern introduction to RNNs and the notation is copied from their book.\n",
    " * M. Baroni, G. Dinu and G. Kruszewski (2014): \"Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors\", Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (ACL2023).[PDF](https://aclanthology.org/P14-1023/) - early indication that the next word predictive models will work in NLP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
