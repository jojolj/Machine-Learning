{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b27855d-a96e-4b70-95fb-d6b52220118a",
   "metadata": {},
   "source": [
    "# Single Neuron Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461b69f1-6603-4b2b-9fce-3fce9f210f27",
   "metadata": {},
   "source": [
    "## Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4d66df-2398-43dd-8c74-3cc892855a82",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f59c98-af38-4c33-b533-a7f503e41de4",
   "metadata": {},
   "source": [
    "**1D linear model**\n",
    "\n",
    "$$\n",
    "y = w_1x+w_0\n",
    "$$\n",
    "\n",
    "where $x \\propto N(\\mu_x, \\sigma_x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780b3df-61cb-4849-b7f0-8de2dfad2538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "N = 50 \n",
    "# Input distribution\n",
    "x = np.random.normal(5.0,1.5,N)\n",
    "w1 = 13.0\n",
    "w0 = 20.0\n",
    "\n",
    "y = w1*x+w0\n",
    "plt.plot(x,y,'ko')\n",
    "plt.title(f'N={N} samples from y={w1}x+{w0}')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.axis([0,13,0,200])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d5cbb-c015-4764-928b-044ebbe1b54b",
   "metadata": {},
   "source": [
    "Fitting a linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c6956-269f-464d-ab22-82719d371ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "\n",
    "# Reshape for the fit() function\n",
    "x = x.reshape(-1,1)\n",
    "\n",
    "model = LinearRegression().fit(x,y)\n",
    "y_hat = model.predict(x)\n",
    "print(f'Estimated model y={np.squeeze(model.coef_):.2f}x+{model.intercept_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1d1d2c-b7c5-41cc-9525-6bb3df657930",
   "metadata": {},
   "source": [
    "**Multi-dimensional regression**\n",
    "\n",
    "In general form $\\mathbf{x} = (x_1, x_2, \\ldots, x_D)^T$. For example, in 2D\n",
    "$$\n",
    " y = w_1x_1+w_2x_2+w_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567b5608-f580-423a-87d2-6ed3c88bce5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = 50 \n",
    "# Input distribution\n",
    "x1 = np.random.normal(5.0,1.5,N)\n",
    "x2 = np.random.normal(3.0,2.0,N)\n",
    "w1 = 13.0\n",
    "w2 = 8.0\n",
    "w0 = 20.0\n",
    "\n",
    "y = w1*x1+w2*x2+w0\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot3D(x1,x2,y,'ko')\n",
    "plt.title(f'N={N} samples from y={w1}x1+{w2}x2+{w0}')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29caea04-0cd2-47f5-a257-13902f0c42f8",
   "metadata": {},
   "source": [
    "Fitting 2D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2c4451-20c8-4ca3-9129-502dbfc019f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for the fit() function\n",
    "X = np.ndarray([N,2])\n",
    "X[:,0] = x1\n",
    "X[:,1] = x2\n",
    "\n",
    "model = LinearRegression().fit(X,y)\n",
    "print(f'Estimated model y={model.coef_[0]:.2f}x1+{model.coef_[1]:.2f}x2+{model.intercept_:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf175b4-c045-42eb-bf05-fd48b3c3b6cd",
   "metadata": {},
   "source": [
    "**Higher order polynomial**\n",
    "\n",
    "$$\n",
    "y = w_1x_1+w_2x_2+w_0 = w_1x+w_2x^2+w_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793ea3c4-e6a3-4b71-9290-7e0740053948",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = 50 \n",
    "# Input distribution\n",
    "x = np.random.normal(5.0,1.5,N)\n",
    "w1 = 0.2\n",
    "w2 = 8.0\n",
    "w0 = 20.0\n",
    "\n",
    "y = w1*x+w2*x**2+w0\n",
    "plt.plot(x,y,'ko')\n",
    "plt.title(f'N={N} samples from y={w1}x1+{w2}x^2+{w0}')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf96f1-5f61-4c30-98c5-104e71566a11",
   "metadata": {},
   "source": [
    "Linear fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d02e00-3705-4143-a260-b9467f77ae79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ndarray([N,2])\n",
    "X[:,0] = x\n",
    "X[:,1] = x**2\n",
    "\n",
    "plt.plot(x,y,'ko')\n",
    "\n",
    "model = LinearRegression().fit(x.reshape(-1,1),y)\n",
    "y_hat = model.predict(x.reshape(-1,1))\n",
    "print(f'Estimated 1D model y={np.squeeze(model.coef_):.2f}x+{model.intercept_:.2f}')\n",
    "plt.plot(range(0,10),model.coef_*range(0,10)+model.intercept_,'r-')\n",
    "\n",
    "model = LinearRegression().fit(X,y)\n",
    "y_hat = model.predict(X)\n",
    "print(f'Estimated 2D model y={model.coef_[0]:.2f}x+{model.coef_[1]:.2f}x^2+{model.intercept_:.2f}')\n",
    "plt.plot(range(0,10),model.coef_[0]*range(0,10)+model.coef_[1]*np.array(range(0,10))**2+model.intercept_,'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1cf6de-63ae-4959-9554-3896bfb987f2",
   "metadata": {},
   "source": [
    "**Noise in linear models**\n",
    "\n",
    "Noise in the model Noisy linear fit (test with N=50,10,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd1de6-f9fd-4921-8191-1efa79d3317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = 1000 \n",
    "mu_n = 0.0\n",
    "sigma_n = 40.0\n",
    "\n",
    "# Input distribution\n",
    "x = np.random.normal(5.0,1.5,N)\n",
    "w1 = 0.2\n",
    "w2 = 8.0\n",
    "w0 = 20.0\n",
    "\n",
    "y = w1*x+w2*x**2+w0+np.random.normal(mu_n,sigma_n,N)\n",
    "plt.plot(x,y,'ko')\n",
    "plt.title(f'N={N} samples from y={w1}x+{w2}x^2+{w0}+N({mu_n},{sigma_n})')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5451094-dcd3-4d21-80ac-f365c92bc941",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ndarray([N,2])\n",
    "X[:,0] = x\n",
    "X[:,1] = x**2\n",
    "\n",
    "model = LinearRegression().fit(X,y)\n",
    "y_hat = model.predict(X)\n",
    "print(f'Estimated 2D model y={model.coef_[0]:.2f}x+{model.coef_[1]:.2f}x^2+{model.intercept_:.2f} (Green is GT)')\n",
    "plt.plot(x,y,'ko')\n",
    "plt.plot(range(0,10),model.coef_[0]*range(0,10)+model.coef_[1]*np.array(range(0,10))**2+model.intercept_,'b-',label='estimated')\n",
    "plt.plot(range(0,10),w1*np.array(range(0,10))+w2*np.array(range(0,10))**2+w0,'g-',label='GT')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae081ac-f38b-40a8-a47d-dbeb028321ba",
   "metadata": {},
   "source": [
    "**Ridge regression**\n",
    "\n",
    "Robustifies linear regression against overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4893af6-1918-4345-9d64-d7eaea97e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X,y)\n",
    "\n",
    "print(f'Estimated 2D model y={model.coef_[0]:.2f}x+{model.coef_[1]:.2f}x^2+{model.intercept_:.2f} (Green is GT)')\n",
    "plt.plot(x,y,'ko')\n",
    "plt.plot(range(0,10),model.coef_[0]*range(0,10)+model.coef_[1]*np.array(range(0,10))**2+model.intercept_,'b-')\n",
    "plt.plot(range(0,10),w1*np.array(range(0,10))+w2*np.array(range(0,10))**2+w0,'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878931d-e1f9-495a-9cd6-3375ceaa61b5",
   "metadata": {},
   "source": [
    "**Handling outliers**\n",
    "\n",
    "RANSAC works when noise and outliers kill linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3b065-b4ca-488e-8de2-62c1fe8b71a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = 50 \n",
    "mu_n = 0.0\n",
    "sigma_n = 40.0\n",
    "\n",
    "# Input distribution\n",
    "x = np.random.normal(5.0,1.5,N)\n",
    "w1 = 0.2\n",
    "w2 = 8.0\n",
    "w0 = 20.0\n",
    "\n",
    "# Outliers\n",
    "x_out = np.random.uniform(0,10,100)\n",
    "y_out = np.random.uniform(0,500,100)\n",
    "\n",
    "y = w1*x+w2*x**2+w0+np.random.normal(mu_n,sigma_n,N)\n",
    "\n",
    "# Add outliers to the data points\n",
    "x_new = np.concatenate((x,x_out),axis=0)\n",
    "y_new = np.concatenate((y,y_out),axis=0)\n",
    "\n",
    "plt.plot(x_new,y_new,'ko')\n",
    "plt.title(f'N={N} samples from y={w1}x+{w2}x^2+{w0}+N({mu_n},{sigma_n})')\n",
    "plt.xlabel('x_1')\n",
    "plt.ylabel('x_2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b05b75d-4896-418c-a7e9-e4445e69b30d",
   "metadata": {},
   "source": [
    "Linear fitting is very sensitive to the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2278ed81-2f82-47fe-84f8-05e7b70656d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ndarray([len(x_new),2])\n",
    "X[:,0] = x_new\n",
    "X[:,1] = x_new**2\n",
    "\n",
    "model = Ridge(alpha=1)\n",
    "model.fit(X,y_new)\n",
    "\n",
    "print(f'Estimated 2D model y={model.coef_[0]:.2f}x+{model.coef_[1]:.2f}x^2+{model.intercept_:.2f} (Green is GT)')\n",
    "plt.plot(x,y,'ko')\n",
    "plt.plot(range(0,10),model.coef_[0]*range(0,10)+model.coef_[1]*np.array(range(0,10))**2+model.intercept_,'b-')\n",
    "plt.plot(range(0,10),w1*np.array(range(0,10))+w2*np.array(range(0,10))**2+w0,'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ea9e35-f457-48d3-a079-3765f2a711bf",
   "metadata": {},
   "source": [
    "RANSAC may works even in the case when vast majority of the points (99%) are outliers. (However you may need to run several times or have a large number of random samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8115ae-f8d9-4805-b28f-e87e8dcaf778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "X = np.ndarray([len(x_new),2])\n",
    "X[:,0] = x_new\n",
    "X[:,1] = x_new**2\n",
    "\n",
    "# Test thresholds 500, 50, 5\n",
    "model = RANSACRegressor(residual_threshold=5, max_trials=1000).fit(X,y_new)\n",
    "\n",
    "print(f'Estimated 2D model y={model.estimator_.coef_[0]:.2f}x+{model.estimator_.coef_[1]:.2f}x^2+{model.estimator_.intercept_:.2f} (Green is GT)')\n",
    "plt.plot(x,y,'ko')\n",
    "plt.plot(range(0,10),model.estimator_.coef_[0]*range(0,10)+model.estimator_.coef_[1]*np.array(range(0,10))**2+model.estimator_.intercept_,'b-')\n",
    "plt.plot(range(0,10),w1*np.array(range(0,10))+w2*np.array(range(0,10))**2+w0,'g-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49417c6-818f-4aba-822c-d1736060a9dd",
   "metadata": {},
   "source": [
    "### Linear classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bb099-1d0f-48af-a49e-5875203353c2",
   "metadata": {},
   "source": [
    "Logistic regression is a standard tool of classification in many fields and especially in medical descision making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38497ef3-75f7-4c83-86e5-39fd11ea152c",
   "metadata": {},
   "source": [
    "Let's make an artificial training set of tumor sizes (in cm) and another indicator whether the tumor is \"cancerous\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e268e88-0e7b-4f50-b1e6-742612922087",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X represents the size of a tumor in centimeters.\n",
    "X = np.array([3.78, 2.44, 2.09, 0.14, 1.72, 1.65, 4.92, 4.37, 4.96, 4.52, 3.69, 5.88]).reshape(-1,1)\n",
    "\n",
    "#Note: X has to be reshaped into a column from a row for the LogisticRegression() function to work.\n",
    "#y represents whether or not the tumor is cancerous (0 for \"No\", 1 for \"Yes\").\n",
    "y = np.array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) \n",
    "\n",
    "X_test = 3.46\n",
    "\n",
    "X_norm = np.squeeze(np.argwhere(y == 0))\n",
    "X_cancer = np.squeeze(np.argwhere(y == 1))\n",
    "\n",
    "plt.plot(X[X_norm],np.zeros(X_norm.shape),'ko',label='normal')\n",
    "plt.plot(X[X_cancer],np.zeros(X_cancer.shape),'rx',label='cancer')\n",
    "plt.plot(X_test,0,'gd',label='?')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85045dd-7cc9-4944-9b3a-1bc30f22b3aa",
   "metadata": {},
   "source": [
    "Run logistic regression to solve $w_0$ and $w_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9919a21-2a07-424a-8ae2-12cb399d8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logr = LogisticRegression()\n",
    "logr.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526a70cf-cd7e-4f86-b0e6-95f5f8d9c17c",
   "metadata": {},
   "source": [
    "Let's test the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b305548-3cb7-4d6e-bcbf-5e77d496f6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict if tumor is cancerous where the size is 3.46mm:\n",
    "predicted = logr.predict(np.array([3.46]).reshape(-1,1))\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6692896f-04fd-485a-8771-438260f50681",
   "metadata": {},
   "source": [
    "Let's study the probabilities of the two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06df531-ee60-49fe-a382-3652856f38b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x  = np.linspace(0,6,num=100)\n",
    "print(logr.intercept_)\n",
    "log_prob = logr.coef_[0] * x + logr.intercept_ \n",
    "print(log_prob)\n",
    "plt.plot(x,1/(1+np.exp(log_prob)),label='Class y=0')\n",
    "plt.plot(x,1-1/(1+np.exp(log_prob)),label='Class y=1')\n",
    "plt.legend()\n",
    "plt.xlabel('Tumor size [cm]')\n",
    "plt.ylabel('Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47375c3-ea5b-45c1-b4b4-f0790e71ab8e",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1804084f-05c7-4715-b2e2-49b8fb79c511",
   "metadata": {},
   "source": [
    "## Gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171bab59-e21a-4978-9ecf-48b83ae17185",
   "metadata": {},
   "source": [
    "Let's define classification data for $x_1~\\lor~x_2$ or $x_1~\\land~x_2$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91725185-1c54-49ff-881d-35e101a54309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "x1 = np.array([0,1,1,0])\n",
    "x2 = np.array([0,0,1,1])\n",
    "\n",
    "#Outputs\n",
    "y = np.array([0,1,1,1]) # OR\n",
    "#y = np.array([0,0,1,0]) # AND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44230ce6-56c5-42a0-a3dd-9f3288f0ca26",
   "metadata": {},
   "source": [
    "Initial weights for $t = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f95233-43ed-4ffb-8e54-4771595a6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_t = 0\n",
    "w2_t = 0\n",
    "w0_t = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a5159-fc19-4a41-b5b9-2dbb7486aaf1",
   "metadata": {},
   "source": [
    "GD algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5c1e7a-3b3b-4c87-82c7-926b7b4c443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit # sigmoid function\n",
    "\n",
    "# Compute MSE heat map for different a and b\n",
    "num_of_epochs = 1000\n",
    "lr = 0.2 # learning rate\n",
    "N = len(x1)\n",
    "\n",
    "for e in range(num_of_epochs):\n",
    "    y_h = expit(w1_t*x1+w2_t*x2+w0_t)\n",
    "    nablaL_w1 = 1/N*sum(2*(y-y_h)*-y_h*(1-y_h)*x1)\n",
    "    nablaL_w2 = 1/N*sum(2*(y-y_h)*-y_h*(1-y_h)*x2)\n",
    "    nablaL_w0 = 1/N*sum(2*(y-y_h)*-y_h*(1-y_h)*1)\n",
    "    #grad_w1 = np.sum(2*x_tr*(y_tr-y)*y*(-1+y))\n",
    "    #grad_w0 = np.sum(2*(y_tr-y)*y*(-1+y))\n",
    "    w1_t = w1_t-lr*nablaL_w1\n",
    "    w2_t = w2_t-lr*nablaL_w2\n",
    "    w0_t = w0_t-lr*nablaL_w0\n",
    "\n",
    "    if np.mod(e,20) == 0 or e == 1: # Plot after every 20th epoch\n",
    "        y_pred = expit(w1_t*x1+w2_t*x2+w0_t)\n",
    "        MSE = np.sum((y-y_pred)**2)/(len(y))\n",
    "        print(MSE)\n",
    "        #plt.title(f'Epoch={e} w0={w0_t:.2f} w1={w1_t:.2f} MSE={MSE:.2f}')\n",
    "        #plt.plot(x_h,y_h,'co', label=\"hobbit\")\n",
    "        #plt.plot(x_e,y_e,'mo', label=\"elf\")\n",
    "        #x = np.linspace(0.0,+5.0,50)\n",
    "        #plt.plot(x,expit(w1_t*x+w0_t),'b-',label='y=logsig(w1x+w0)')\n",
    "        #plt.plot([0.5, 5.0],[0.5,0.5],'k--',label='y=0 (class boundary)')\n",
    "        #plt.xlabel('height [m]')\n",
    "        #plt.legend()\n",
    "        #plt.show()\n",
    "\n",
    "np.set_printoptions(precision=3,suppress=True)\n",
    "print(f'True values y={y} and predicted values y_pred={y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4c081e-7043-4917-ac15-cb9b0e30033c",
   "metadata": {},
   "source": [
    "Plot decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ef3d42-fe11-4d23-894e-b34d2085cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "\n",
    "feature_1, feature_2 = np.meshgrid(np.linspace(-0.1,1.1), np.linspace(-0.1,1.1))\n",
    "grid = np.vstack([feature_1.ravel(), feature_2.ravel()]).T\n",
    "print('Decision boundary')\n",
    "y_pred = np.reshape(expit(w1_t*grid[:,0]+w2_t*grid[:,1]+w0_t), feature_1.shape)\n",
    "display = DecisionBoundaryDisplay(xx0=feature_1, xx1=feature_2, response=y_pred)\n",
    "display.plot()\n",
    "#display.ax_.scatter(\n",
    "#...     iris.data[:, 0], iris.data[:, 1], c=iris.target, edgecolor=\"black\"\n",
    "#... )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4985783f-5f7d-47d4-bebf-197c12b71941",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    " * C.M. Bishop (2006): Pattern Recognition and Machine Learning, Chapters 3 and 4 ([PDF](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf)) - Bishop provides excellent introduction to linear regression and classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
